import os
import yaml
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from pathlib import Path
import psutil
import cv2
import numpy as np
from tqdm import tqdm
from einops import rearrange, repeat
from einops.layers.torch import Rearrange
import shutil


# ============================== å…¨å±€é…ç½®ï¼ˆæ‰€æœ‰å‚æ•°é›†ä¸­ä¿®æ”¹æ­¤å¤„ï¼‰==============================
# 1. è·¯å¾„é…ç½®ï¼ˆæ ¹æ®è‡ªå·±çš„ç›®å½•è°ƒæ•´ï¼‰
DATASET_ROOT = r"E:\2025CompetitionLog\9.9\tRVALCUP_2\ChinaMoblieCup\dataset"               # æ•°æ®é›†æ ¹ç›®å½•ï¼ˆéœ€åŒ…å«images/trainã€labels/valç­‰å­ç›®å½•ï¼‰
TRAIN_OUTPUT_DIR = "./runs/vit_detect"   # è®­ç»ƒç»“æœä¿å­˜ç›®å½•ï¼ˆæƒé‡ã€æ—¥å¿—ç­‰ï¼‰
MODEL_NAME = "vit_detector_single_gpu"   # å®éªŒåç§°ï¼ˆåŒºåˆ†ä¸åŒè®­ç»ƒä»»åŠ¡ï¼‰
BEST_MODEL_PATH = os.path.join(TRAIN_OUTPUT_DIR, MODEL_NAME, "weights", "best.pt")  # æœ€ä½³æƒé‡è·¯å¾„
VAL_IMAGES_PATH = os.path.join(DATASET_ROOT, "images", "val")  # éªŒè¯é›†å›¾åƒè·¯å¾„
VAL_LABELS_PATH = os.path.join(DATASET_ROOT, "labels", "val")  # éªŒè¯é›†æ ‡ç­¾è·¯å¾„

# 2. æ¨¡å‹è¶…å‚æ•°ï¼ˆå•GPUæ˜¾å­˜å‹å¥½å‹é…ç½®ï¼‰
IMG_SIZE = (832, 1472)  # è¾“å…¥å›¾åƒå°ºå¯¸ (é«˜, å®½)ï¼Œéœ€èƒ½è¢«PATCH_SIZEæ•´é™¤
PATCH_SIZE = 32         # ViTå›¾åƒåˆ†å—å¤§å°ï¼ˆ32Ã—32åƒç´ /å—ï¼‰
DIM = 512               # Transformerç‰¹å¾ç»´åº¦ï¼ˆåŸ1024ï¼Œå•å¡é™ä¸º512å‡å°‘æ˜¾å­˜å ç”¨ï¼‰
DEPTH = 4               # Transformerç¼–ç å™¨å±‚æ•°ï¼ˆåŸ6ï¼Œå•å¡é™ä¸º4é™ä½è®¡ç®—é‡ï¼‰
HEADS = 8               # å¤šå¤´æ³¨æ„åŠ›å¤´æ•°ï¼ˆéœ€æ•´é™¤DIMï¼Œ512Ã·8=64ï¼Œç¬¦åˆæ³¨æ„åŠ›æœºåˆ¶è¦æ±‚ï¼‰
MLP_DIM = 1024          # MLPéšè—å±‚ç»´åº¦ï¼ˆåŸ2048ï¼Œå•å¡å‡åŠï¼‰
NUM_CLASSES = 4         # ç›®æ ‡ç±»åˆ«æ•°ï¼ˆship/people/car/motorï¼‰

# 3. è®­ç»ƒè¶…å‚æ•°ï¼ˆå•å¡é€‚é…ï¼‰
EPOCHS = 200            # è®­ç»ƒè½®æ¬¡ï¼ˆåŸ300ï¼Œå•å¡é€‚å½“å‡å°‘ä»¥ç¼©çŸ­æ—¶é—´ï¼‰
BATCH_SIZE = 4          # å•å¡æ‰¹æ¬¡å¤§å°ï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼š12GBæ˜¾å­˜å»ºè®®2-4ï¼Œ24GBå»ºè®®4-8ï¼‰
BASE_LR = 5e-5          # åŸºç¡€å­¦ä¹ ç‡ï¼ˆå•å¡æ— éœ€ç¼©æ”¾ï¼ŒåŸ1e-4æ˜“éœ‡è¡ï¼‰
WEIGHT_DECAY = 1e-4     # æƒé‡è¡°å‡ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
WARMUP_EPOCHS = 3       # å­¦ä¹ ç‡é¢„çƒ­è½®æ¬¡ï¼ˆå•å¡æ— éœ€é•¿é¢„çƒ­ï¼‰
LOSS_LAMBDA_BOX = 5.0   # è¾¹ç•Œæ¡†æŸå¤±æƒé‡ï¼ˆçªå‡ºä½ç½®é¢„æµ‹ç²¾åº¦ï¼‰
LOSS_LAMBDA_CLS = 1.0   # ç±»åˆ«æŸå¤±æƒé‡

# 4. æ¨ç†è¶…å‚æ•°
CONF_THRESHOLD = 0.5    # ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆè¿‡æ»¤ä½ç½®ä¿¡åº¦é¢„æµ‹æ¡†ï¼‰
IOU_THRESHOLD = 0.5     # NMSäº¤å¹¶æ¯”é˜ˆå€¼ï¼ˆå»é™¤é‡å¤æ¡†ï¼‰

# 5. è®¾å¤‡é…ç½®ï¼ˆè‡ªåŠ¨é€‚é…å•GPU/CPUï¼‰
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
GPU_MEMORY_GB = torch.cuda.get_device_properties(0).total_memory / (1024**3) if torch.cuda.is_available() else 0
# =====================================================================================


# ============================== æ ¸å¿ƒå·¥å…·å‡½æ•°ï¼ˆé¿å…å¤–éƒ¨ä¾èµ–ï¼‰==============================
def nms_boxes(boxes, iou_threshold):
    """å†…ç½®NMSå‡½æ•°ï¼ˆæ— éœ€ä¾èµ–vote_config.pyï¼‰ï¼šå»é™¤é‡å¤é¢„æµ‹æ¡†"""
    if len(boxes) == 0:
        return []
    
    # æŒ‰ç½®ä¿¡åº¦é™åºæ’åº
    boxes = sorted(boxes, key=lambda x: x[4], reverse=True)
    keep_boxes = []
    
    while boxes:
        current_box = boxes.pop(0)
        keep_boxes.append(current_box)
        
        # è®¡ç®—å½“å‰æ¡†ä¸å‰©ä½™æ¡†çš„IoUï¼Œè¿‡æ»¤è¶…è¿‡é˜ˆå€¼çš„æ¡†
        boxes = [
            box for box in boxes
            if calculate_iou(np.array(current_box[:4]), np.array(box[:4])) < iou_threshold
        ]
    
    return keep_boxes

def calculate_iou(box1, box2):
    """è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„IoUï¼ˆäº¤å¹¶æ¯”ï¼‰"""
    x1_max = max(box1[0], box2[0])
    y1_max = max(box1[1], box2[1])
    x2_min = min(box1[2], box2[2])
    y2_min = min(box1[3], box2[3])
    
    if x2_min <= x1_max or y2_min <= y1_max:
        return 0.0
    
    intersection = (x2_min - x1_max) * (y2_min - y1_max)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - intersection
    
    return intersection / union if union > 0 else 0.0


# ============================== æ¨¡å‹å®šä¹‰ï¼ˆViTç‰¹å¾æå– + æ£€æµ‹å¤´ï¼‰==============================
class ViTBackbone(nn.Module):
    """Vision Transformerç‰¹å¾æå–å™¨ï¼šå°†å›¾åƒâ†’åˆ†å—â†’åµŒå…¥â†’Transformerç¼–ç """
    def __init__(self, img_size=IMG_SIZE, patch_size=PATCH_SIZE, dim=DIM, depth=DEPTH, heads=HEADS, mlp_dim=MLP_DIM):
        super().__init__()
        image_height, image_width = img_size
        patch_height, patch_width = patch_size, patch_size
        
        # è®¡ç®—è¡¥ä¸æ•°é‡å’Œå•ä¸ªè¡¥ä¸ç»´åº¦ï¼ˆ3é€šé“Ã—è¡¥ä¸é«˜Ã—è¡¥ä¸å®½ï¼‰
        self.num_patches = (image_height // patch_height) * (image_width // patch_width)
        self.patch_dim = 3 * patch_height * patch_width
        
        # 1. å›¾åƒåˆ†å— + çº¿æ€§åµŒå…¥ï¼ˆå°†æ¯ä¸ª3Ã—32Ã—32çš„è¡¥ä¸è½¬ä¸º512ç»´ç‰¹å¾ï¼‰
        self.to_patch_embedding = nn.Sequential(
            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width),
            nn.Linear(self.patch_dim, dim),
        )
        
        # 2. ä½ç½®ç¼–ç ï¼ˆå¯å­¦ä¹ ï¼Œç»™æ¯ä¸ªè¡¥ä¸æ·»åŠ "ä½ç½®ä¿¡æ¯"ï¼Œé¿å…Transformerå¿½ç•¥é¡ºåºï¼‰
        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, dim))  # +1æ˜¯cls_token
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))  # åˆ†ç±»tokenï¼ˆæ•´åˆå…¨å±€ç‰¹å¾ï¼‰
        
        # 3. Transformerç¼–ç å™¨ï¼ˆå †å 4å±‚ï¼Œæ¯å±‚å«å¤šå¤´æ³¨æ„åŠ›+MLPï¼Œæå–å…¨å±€å…³è”ç‰¹å¾ï¼‰
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=dim,
                nhead=heads,
                dim_feedforward=mlp_dim,
                dropout=0.1,
                batch_first=True  # è¾“å…¥æ ¼å¼ï¼š(batch, è¡¥ä¸æ•°, ç‰¹å¾ç»´åº¦)
            ),
            num_layers=depth
        )

    def forward(self, img):
        # imgè¾“å…¥æ ¼å¼ï¼š(batch, 3, 832, 1472)
        batch_size = img.shape[0]
        
        # æ­¥éª¤1ï¼šç”Ÿæˆè¡¥ä¸åµŒå…¥ï¼ˆ(batch, 3, 832, 1472) â†’ (batch, 1176, 512)ï¼Œ1176=26Ã—45ï¼Œå³832/32=26ï¼Œ1472/32=45ï¼‰
        x = self.to_patch_embedding(img)
        
        # æ­¥éª¤2ï¼šæ·»åŠ cls_tokenï¼ˆåœ¨è¡¥ä¸ç‰¹å¾å‰æ‹¼æ¥ï¼Œæ ¼å¼å˜ä¸º(batch, 1177, 512)ï¼‰
        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=batch_size)
        x = torch.cat([cls_tokens, x], dim=1)
        
        # æ­¥éª¤3ï¼šæ·»åŠ ä½ç½®ç¼–ç ï¼ˆç»™æ¯ä¸ªè¡¥ä¸+cls_tokenæ³¨å…¥ä½ç½®ä¿¡æ¯ï¼‰
        x += self.pos_embedding
        
        # æ­¥éª¤4ï¼šTransformerç¼–ç ï¼ˆæå–å…¨å±€ç‰¹å¾ï¼Œè¾“å‡ºæ ¼å¼ä¸å˜ï¼‰
        x = self.transformer(x)
        
        return x  # è¿”å›å«cls_tokençš„ç‰¹å¾ï¼š(batch, 1177, 512)


class ViTDetector(nn.Module):
    """åŸºäºViTçš„ç›®æ ‡æ£€æµ‹å™¨ï¼šBackboneæç‰¹å¾ + æ£€æµ‹å¤´é¢„æµ‹æ¡†/ç±»åˆ«"""
    def __init__(self, num_classes=NUM_CLASSES, img_size=IMG_SIZE, patch_size=PATCH_SIZE, dim=DIM, depth=DEPTH, heads=HEADS):
        super().__init__()
        self.num_classes = num_classes
        self.img_size = img_size
        self.patch_size = patch_size
        
        # 1. ç‰¹å¾æå–ï¼ˆè°ƒç”¨ViT Backboneï¼‰
        self.backbone = ViTBackbone(img_size=img_size, patch_size=patch_size, dim=dim, depth=depth, heads=heads)
        
        # 2. æ£€æµ‹å¤´ï¼ˆå°†æ¯ä¸ªè¡¥ä¸çš„512ç»´ç‰¹å¾â†’é¢„æµ‹6ä¸ªå€¼ï¼š4åæ ‡+1ç½®ä¿¡åº¦+4ç±»åˆ«ï¼‰
        self.detection_head = nn.Sequential(
            nn.Linear(dim, dim // 2),  # é™ç»´ï¼š512â†’256ï¼Œå‡å°‘è®¡ç®—é‡
            nn.GELU(),                 # æ¿€æ´»å‡½æ•°ï¼ˆæ¯”ReLUæ›´é€‚åˆTransformerï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼‰
            nn.Dropout(0.2),           #  dropoutï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
            nn.Linear(dim // 2, 5 + num_classes)  # è¾“å‡ºå±‚ï¼š5(æ¡†+ç½®ä¿¡åº¦)+4(ç±»åˆ«)=9ç»´
        )
        
        # æƒé‡åˆå§‹åŒ–ï¼ˆä¿è¯è®­ç»ƒç¨³å®šæ€§ï¼‰
        self._initialize_weights()

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.trunc_normal_(m.weight, std=.02)  # æˆªæ–­æ­£æ€åˆ†å¸ƒï¼šé¿å…æƒé‡è¿‡å¤§
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.bias, 0)
                nn.init.constant_(m.weight, 1.0)  # LayerNormæƒé‡åˆå§‹åŒ–ä¸º1

    def forward(self, x):
        # xè¾“å…¥æ ¼å¼ï¼š(batch, 3, 832, 1472)
        batch_size = x.shape[0]
        h, w = self.img_size
        
        # æ­¥éª¤1ï¼šè·å–ViTç‰¹å¾ï¼Œå»æ‰cls_tokenï¼ˆåªä¿ç•™è¡¥ä¸ç‰¹å¾ï¼š(batch, 1176, 512)ï¼‰
        features = self.backbone(x)
        patch_features = features[:, 1:]
        
        # æ­¥éª¤2ï¼šæ£€æµ‹å¤´é¢„æµ‹ï¼ˆæ¯ä¸ªè¡¥ä¸è¾“å‡º(5 + num_classes)ç»´åŸå§‹å€¼ï¼š(batch, 1176, 5+num_classes)ï¼‰
        detections = self.detection_head(patch_features)  # åŸå§‹è¾“å‡ºï¼ˆæœªç»sigmoid/softmax/åæ ‡è½¬æ¢ï¼‰
        
        # ä»¥ä¸‹ä¸ºæ¨ç†/å±•ç¤ºç”¨çš„åå¤„ç†ï¼ˆä¸ä¹‹å‰ä»£ç ä¸€è‡´ï¼‰
        bboxes = detections[..., :4]  # åŸå§‹bboxå‚æ•°
        confidences = torch.sigmoid(detections[..., 4:5])  # ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
        class_logits = detections[..., 5:]  # ç±»åˆ«logits
        
        # ç›¸å¯¹å‚æ•° -> ç»å¯¹åæ ‡ï¼ˆä¸ä¹‹å‰ä¸€è‡´ï¼‰
        bboxes[..., 0] = (bboxes[..., 0] * 2 - 1) * w / 2  # x_center
        bboxes[..., 1] = (bboxes[..., 1] * 2 - 1) * h / 2  # y_center
        bboxes[..., 2] = torch.exp(bboxes[..., 2]) * (w / (w // self.patch_size))  # width
        bboxes[..., 3] = torch.exp(bboxes[..., 3]) * (h / (h // self.patch_size))  # height
        
        x1 = bboxes[..., 0] - bboxes[..., 2] / 2
        y1 = bboxes[..., 1] - bboxes[..., 3] / 2
        x2 = bboxes[..., 0] + bboxes[..., 2] / 2
        y2 = bboxes[..., 1] + bboxes[..., 3] / 2
        
        class_ids = torch.argmax(F.softmax(class_logits, dim=-1), dim=-1, keepdim=True)
        class_scores = torch.max(F.softmax(class_logits, dim=-1), dim=-1, keepdim=True).values
        final_confidence = confidences * class_scores
        
        processed_outputs = torch.cat([x1.unsqueeze(-1), y1.unsqueeze(-1), x2.unsqueeze(-1), y2.unsqueeze(-1), final_confidence, class_ids.float()], dim=-1)
        
        # è¿”å›ï¼š (processed_outputs_for_inference, raw_detection_head_outputs_for_loss)
        return processed_outputs, detections


# ============================== æŸå¤±å‡½æ•°ï¼ˆè¾¹ç•Œæ¡†+ç±»åˆ«+ç½®ä¿¡åº¦è”åˆä¼˜åŒ–ï¼‰==============================
class DetectionLoss(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES, lambda_box=LOSS_LAMBDA_BOX, lambda_cls=LOSS_LAMBDA_CLS):
        super().__init__()
        self.num_classes = num_classes
        self.lambda_box = lambda_box  # è¾¹ç•Œæ¡†æŸå¤±æƒé‡
        self.lambda_cls = lambda_cls  # ç±»åˆ«æŸå¤±æƒé‡
        
        # åŸºç¡€æŸå¤±å‡½æ•°
        self.bbox_loss = nn.SmoothL1Loss(reduction='sum')  # è¾¹ç•Œæ¡†æŸå¤±ï¼ˆå¯¹å¼‚å¸¸å€¼é²æ£’ï¼Œæ¯”MSEå¥½ï¼‰
        self.class_loss = nn.CrossEntropyLoss(reduction='sum')  # ç±»åˆ«æŸå¤±
        self.confidence_loss = nn.BCEWithLogitsLoss(reduction='sum')  # ç½®ä¿¡åº¦æŸå¤±ï¼ˆäºŒåˆ†ç±»ï¼šæ˜¯/å¦ç›®æ ‡ï¼‰

    def forward(self, predictions, targets):
        """
        predictionsï¼šåŸå§‹æ£€æµ‹å¤´è¾“å‡º (batch, 1176, 5+num_classes)
        targetsï¼šåˆ—è¡¨ â†’ æ¯ä¸ªå…ƒç´ æ˜¯(num_objects, 5) â†’ çœŸå®ç›®æ ‡ï¼ˆx1,y1,x2,y2,ç±»åˆ«IDï¼‰ï¼Œåæ ‡ä¸ºç»å¯¹åƒç´ ï¼ˆä¸ dataset ä¸­ä¸€è‡´ï¼‰
        """
        total_loss = 0.0
        batch_size = predictions.shape[0]
        
        for i in range(batch_size):
            pred = predictions[i]  # (1176, 5+num_classes) åŸå§‹è¾“å‡º
            target = targets[i]    # (num_objects, 5)
            
            # æƒ…å†µ1ï¼šæ— çœŸå®ç›®æ ‡ â†’ åªä¼˜åŒ–ç½®ä¿¡åº¦ï¼ˆå¸Œæœ›ä¸º0ï¼‰
            if target.numel() == 0:
                # pred[:,4] ä¸ºç½®ä¿¡åº¦ logitï¼ˆBCEWithLogitsLoss ç›´æ¥ä½¿ç”¨ï¼‰
                conf_loss = self.confidence_loss(pred[:, 4], torch.zeros_like(pred[:, 4]))
                total_loss += conf_loss
                continue
            
            # å…ˆå°†åŸå§‹é¢„æµ‹çš„ bbox å‚æ•° è½¬æ¢ä¸º ä¸ processed_outputs ç›¸åŒçš„ ç»å¯¹åæ ‡æ ¼å¼ x1,y1,x2,y2
            # pred_bbox_params: (1176,4) -> ä¸ ViTDetector.forward çš„åŒæ ·å˜æ¢
            pred_bbox = pred[:, :4].clone()
            # æ³¨æ„ï¼špred_bbox ä½¿ç”¨ç›¸åŒçš„æ•°å­¦å˜æ¢ï¼ˆç›¸å¯¹->ç»å¯¹ï¼‰
            w_img, h_img = self._get_img_wh()  # helper below uses global IMG_SIZE
            # x_center, y_center, w_pred, h_pred
            pred_bbox[:, 0] = (pred_bbox[:, 0] * 2 - 1) * w_img / 2
            pred_bbox[:, 1] = (pred_bbox[:, 1] * 2 - 1) * h_img / 2
            pred_bbox[:, 2] = torch.exp(pred_bbox[:, 2]) * (w_img / (w_img // PATCH_SIZE))
            pred_bbox[:, 3] = torch.exp(pred_bbox[:, 3]) * (h_img / (h_img // PATCH_SIZE))
            
            # è½¬æ¢ä¸º x1,y1,x2,y2 æ ¼å¼ï¼ˆä¸ target ä¸€è‡´ï¼‰
            pred_x1 = pred_bbox[:, 0] - pred_bbox[:, 2] / 2
            pred_y1 = pred_bbox[:, 1] - pred_bbox[:, 3] / 2
            pred_x2 = pred_bbox[:, 0] + pred_bbox[:, 2] / 2
            pred_y2 = pred_bbox[:, 1] + pred_bbox[:, 3] / 2
            
            pred_boxes_abs = torch.stack([pred_x1, pred_y1, pred_x2, pred_y2], dim=1)  # (1176,4)
            
            # è®¡ç®— IoU çŸ©é˜µï¼ˆé¢„æµ‹æ¡† vs çœŸå®ç›®æ ‡ï¼‰ï¼Œä¾›åŒ¹é…ä½¿ç”¨
            num_preds = pred_boxes_abs.shape[0]
            num_tgts = target.shape[0]
            ious = torch.zeros((num_preds, num_tgts), device=pred.device)
            for p_idx in range(num_preds):
                for t_idx in range(num_tgts):
                    # åŸèµ‹å€¼ä¼šäº§ç”Ÿ numpy.float32 -> CUDA tensor ç±»å‹ä¸å…¼å®¹é”™è¯¯
                    # ä¿®å¤ï¼šå…ˆè½¬æ¢ä¸º Python float å†èµ‹å€¼
                    p_box_np = pred_boxes_abs[p_idx].detach().cpu().numpy()
                    t_box_np = target[t_idx, :4].detach().cpu().numpy()
                    val = float(calculate_iou(p_box_np, t_box_np))
                    ious[p_idx, t_idx] = val
            
            # æ¯ä¸ªçœŸå®ç›®æ ‡åŒ¹é…IoUæœ€å¤§çš„é¢„æµ‹æ¡†ï¼ˆä½œä¸ºæ­£æ ·æœ¬ï¼‰
            matched_preds = torch.argmax(ious, dim=0)  # (num_tgts,)
            matched_targets = torch.arange(num_tgts, device=pred.device)
            
            # ç”Ÿæˆæ­£è´Ÿæ ·æœ¬æ©ç ï¼ˆç”¨äºç½®ä¿¡åº¦æŸå¤±ï¼‰
            positive_mask = torch.zeros(num_preds, dtype=torch.bool, device=pred.device)
            positive_mask[matched_preds] = True
            
            # ä½¿ç”¨ matched_preds æŒ‰é¡ºåºå–å‡ºå¯¹åº”çš„é¢„æµ‹ï¼ˆä¿è¯ä¸ target ä¸€ä¸€å¯¹åº”ï¼‰
            pred_matched_boxes = pred_boxes_abs[matched_preds]        # (num_tgts,4)
            target_matched_boxes = target[matched_targets, :4]        # (num_tgts,4)
            
            # è¾¹ç•Œæ¡†æŸå¤±ï¼ˆä»…æ­£æ ·æœ¬ï¼‰
            box_loss = self.bbox_loss(pred_matched_boxes, target_matched_boxes)
            
            # ç±»åˆ«æŸå¤±ï¼šä½¿ç”¨åŸå§‹ class logitsï¼ˆpred[..., 5:5+num_classes]ï¼‰ï¼Œä¸ç»è¿‡ argmax
            class_logits = pred[matched_preds, 5:5+self.num_classes]  # (num_tgts, num_classes)
            class_targets = target[matched_targets, 4].long()        # (num_tgts,)
            cls_loss = self.class_loss(class_logits, class_targets)
            
            # ç½®ä¿¡åº¦æŸå¤±ï¼šä½¿ç”¨ç½®ä¿¡åº¦ logitï¼ˆpred[:,4]ï¼‰ï¼Œæ­£è´Ÿæ ·æœ¬å‡å‚ä¸
            conf_targets = torch.zeros(num_preds, device=pred.device)
            conf_targets[positive_mask] = 1.0
            conf_loss = self.confidence_loss(pred[:, 4], conf_targets)
            
            total_loss += self.lambda_box * box_loss + self.lambda_cls * cls_loss + conf_loss
        
        return total_loss / batch_size  # æ‰¹æ¬¡å¹³å‡æŸå¤±

    # helperï¼šè¿”å›å›¾åƒå®½é«˜ï¼ˆä¸æ¨¡å‹å’Œ dataset ä¿æŒä¸€è‡´ï¼‰
    def _get_img_wh(self):
        # IMG_SIZE ä¸º (H, W)
        return IMG_SIZE[1], IMG_SIZE[0]


# ============================== æ•°æ®é›†ï¼ˆåŠ è½½YOLOæ ¼å¼æ•°æ®ï¼‰==============================
class CustomDetectionDataset(Dataset):
    """è‡ªå®šä¹‰æ•°æ®é›†ï¼šåŠ è½½å›¾åƒ+YOLOæ ¼å¼txtæ ‡ç­¾ï¼Œè‡ªåŠ¨é€‚é…è¾“å…¥å°ºå¯¸"""
    def __init__(self, img_dir, label_dir, img_size=IMG_SIZE, transform=None):
        self.img_dir = Path(img_dir)
        self.label_dir = Path(label_dir)
        self.img_size = img_size
        
        # é»˜è®¤æ•°æ®é¢„å¤„ç†ï¼ˆå¯æ‰©å±•ï¼‰
        self.transform = transform or transforms.Compose([
            transforms.ToTensor(),  # HWCâ†’CHWï¼Œåƒç´ å€¼å½’ä¸€åŒ–åˆ°[0,1]
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  #  imagenetæ ‡å‡†åŒ–
        ])
        
        # åŠ è½½æ‰€æœ‰å›¾åƒè·¯å¾„ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
        self.images = []
        for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
            self.images.extend(list(self.img_dir.glob(f'*{ext}')))
            self.images.extend(list(self.img_dir.glob(f'*{ext.upper()}')))

    def __len__(self):
        return len(self.images)
        
    def __getitem__(self, idx):
        img_path = self.images[idx]
        label_path = self.label_dir / (img_path.stem + '.txt')  # æ ‡ç­¾æ–‡ä»¶ä¸å›¾åƒåŒå
        
        # æ­¥éª¤1ï¼šåŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ
        image = cv2.imread(str(img_path))
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # BGRâ†’RGBï¼ˆcv2é»˜è®¤BGRï¼ŒPyTorché»˜è®¤RGBï¼‰
        h_origin, w_origin = image.shape[:2]
        image = cv2.resize(image, (self.img_size[1], self.img_size[0]))  # ç¼©æ”¾åˆ°832Ã—1472
        
        # æ­¥éª¤2ï¼šåŠ è½½å¹¶è½¬æ¢æ ‡ç­¾ï¼ˆYOLOç›¸å¯¹åæ ‡â†’ç»å¯¹åæ ‡ï¼‰
        targets = []
        if label_path.exists():
            with open(label_path, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) != 5:
                        continue  # è·³è¿‡æ ¼å¼é”™è¯¯çš„è¡Œ
                    
                    cls_id, cx_rel, cy_rel, bw_rel, bh_rel = map(float, parts)
                    
                    # ç›¸å¯¹åæ ‡â†’åŸå§‹å›¾åƒç»å¯¹åæ ‡
                    x1_origin = (cx_rel - bw_rel/2) * w_origin
                    y1_origin = (cy_rel - bh_rel/2) * h_origin
                    x2_origin = (cx_rel + bw_rel/2) * w_origin
                    y2_origin = (cy_rel + bh_rel/2) * h_origin
                    
                    # é€‚é…åˆ°ç¼©æ”¾åçš„å›¾åƒå°ºå¯¸
                    x1 = x1_origin * self.img_size[1] / w_origin
                    y1 = y1_origin * self.img_size[0] / h_origin
                    x2 = x2_origin * self.img_size[1] / w_origin
                    y2 = y2_origin * self.img_size[0] / h_origin
                    
                    targets.append([x1, y1, x2, y2, cls_id])
        
        # æ­¥éª¤3ï¼šåº”ç”¨æ•°æ®å¢å¼ºï¼ˆå¦‚å®šä¹‰äº†transformï¼‰
        if self.transform:
            image = self.transform(image)
            
        return image, torch.tensor(targets, dtype=torch.float32)


# ============================== è®­ç»ƒå™¨ï¼ˆå•å¡è®­ç»ƒé€»è¾‘ï¼‰==============================
class ViTDetectorTrainer:
    def __init__(self, dataset_root=DATASET_ROOT):
        self.dataset_root = Path(dataset_root)
        self.data_yaml = self.dataset_root / 'data.yaml'
        self.class_names = ['ship', 'people', 'car', 'motor']  # ç±»åˆ«åç§°
        self.num_classes = NUM_CLASSES
        
        # éªŒè¯è®¾å¤‡å’Œæ•°æ®é›†ï¼ˆå•å¡å‹å¥½ï¼‰
        self.validate_device()
        self.validate_dataset_structure()

    def validate_device(self):
        """éªŒè¯å•å¡/CPUç¯å¢ƒ"""
        print("=" * 60)
        print("ğŸ–¥ï¸ è®¾å¤‡ç¯å¢ƒéªŒè¯")
        print("=" * 60)
        if torch.cuda.is_available():
            print(f"âœ… æ£€æµ‹åˆ°å•GPUï¼š{torch.cuda.get_device_name(0)}")
            print(f"   æ˜¾å­˜å®¹é‡ï¼š{GPU_MEMORY_GB:.1f} GB")
        else:
            print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®ä¼˜å…ˆé…ç½®GPUï¼‰")
        print(f"   æœ€ç»ˆä½¿ç”¨è®¾å¤‡ï¼š{DEVICE}")

    def validate_dataset_structure(self):
        """éªŒè¯æ•°æ®é›†ç›®å½•ç»“æ„ï¼ˆç¡®ä¿ç¬¦åˆè¦æ±‚ï¼‰"""
        required_dirs = [
            self.dataset_root / 'images' / 'train',
            self.dataset_root / 'images' / 'val',
            self.dataset_root / 'labels' / 'train',
            self.dataset_root / 'labels' / 'val'
        ]
        
        print("\n" + "=" * 60)
        print("ğŸ“ æ•°æ®é›†ç»“æ„éªŒè¯")
        print("=" * 60)
        for dir_path in required_dirs:
            if not dir_path.exists():
                raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨ï¼š{dir_path}ï¼ˆè¯·æ£€æŸ¥æ•°æ®é›†è·¯å¾„ï¼‰")
            print(f"âœ“ {dir_path}")
        
        # è‡ªåŠ¨åˆ›å»ºdata.yamlï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰
        if not self.data_yaml.exists():
            print(f"âš  data.yamlä¸å­˜åœ¨ï¼Œå°†è‡ªåŠ¨åˆ›å»ºï¼š{self.data_yaml}")
            self.create_data_yaml()
        else:
            print(f"âœ“ {self.data_yaml}")

    def create_data_yaml(self):
        """åˆ›å»ºæ•°æ®é›†é…ç½®æ–‡ä»¶"""
        dataset_config = {
            'train': './images/train',
            'val': './images/val',
            'nc': self.num_classes,
            'names': self.class_names
        }
        with open(self.data_yaml, 'w', encoding='utf-8') as f:
            yaml.dump(dataset_config, f, default_flow_style=False, allow_unicode=True)

    def train_model(self):
        """å•å¡è®­ç»ƒä¸»é€»è¾‘"""
        print("\n" + "=" * 80)
        print(f"ğŸš€ å¼€å§‹ViTç›®æ ‡æ£€æµ‹è®­ç»ƒï¼ˆå•å¡/CPUï¼‰")
        print(f"ğŸ“ è¾“å…¥å°ºå¯¸ï¼š{IMG_SIZE[0]}Ã—{IMG_SIZE[1]} | ğŸ“¦ æ‰¹æ¬¡å¤§å°ï¼š{BATCH_SIZE} | ğŸ”„ è®­ç»ƒè½®æ¬¡ï¼š{EPOCHS}")
        print("=" * 80)

        # 1. åˆ›å»ºæ¨¡å‹å¹¶ç§»è‡³è®¾å¤‡
        model = ViTDetector().to(DEVICE)
        print(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ˆå‚æ•°æ€»é‡ï¼š{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.1f}Mï¼‰")

        # 2. åŠ è½½æ•°æ®é›†
        train_dataset = CustomDetectionDataset(
            img_dir=self.dataset_root / 'images' / 'train',
            label_dir=self.dataset_root / 'labels' / 'train'
        )
        val_dataset = CustomDetectionDataset(
            img_dir=self.dataset_root / 'images' / 'val',
            label_dir=self.dataset_root / 'labels' / 'val'
        )
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=BATCH_SIZE,
            shuffle=True,
            num_workers=min(4, psutil.cpu_count() // 2),  # å•å¡æ— éœ€å¤šworkerï¼ˆé¿å…CPUç“¶é¢ˆï¼‰
            collate_fn=self.collate_fn,
            pin_memory=True if torch.cuda.is_available() else False  #  pinned memoryåŠ é€ŸGPUæ•°æ®ä¼ è¾“
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=BATCH_SIZE,
            shuffle=False,
            num_workers=min(4, psutil.cpu_count() // 2),
            collate_fn=self.collate_fn,
            pin_memory=True if torch.cuda.is_available() else False
        )
        print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼ˆè®­ç»ƒé›†ï¼š{len(train_dataset)}å¼  | éªŒè¯é›†ï¼š{len(val_dataset)}å¼ ï¼‰")

        # 3. åˆå§‹åŒ–ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°ã€å­¦ä¹ ç‡è°ƒåº¦
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=BASE_LR,
            weight_decay=WEIGHT_DECAY
        )
        criterion = DetectionLoss()
        # ä½™å¼¦é€€ç«è°ƒåº¦ï¼ˆæ¯”å›ºå®šå­¦ä¹ ç‡æ”¶æ•›æ›´ç¨³å®šï¼‰
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=EPOCHS,
            eta_min=BASE_LR * 0.01  # æœ€ç»ˆå­¦ä¹ ç‡ä¸ºåŸºç¡€çš„1%
        )

        # 4. åˆ›å»ºä¿å­˜ç›®å½•
        save_dir = Path(TRAIN_OUTPUT_DIR) / MODEL_NAME
        weights_dir = save_dir / 'weights'
        weights_dir.mkdir(parents=True, exist_ok=True)
        print(f"âœ… ä¿å­˜ç›®å½•åˆ›å»ºå®Œæˆï¼š{save_dir}")

        # 5. è®­ç»ƒå¾ªç¯
        best_map = 0.0
        for epoch in range(EPOCHS):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [è®­ç»ƒ]")
            
            for images, targets in progress_bar:
                images = images.to(DEVICE)
                targets = [t.to(DEVICE) for t in targets]
                
                optimizer.zero_grad()
                # æ³¨æ„ï¼šmodel è¿”å› (processed_outputs, raw_detections)
                processed_outputs, raw_outputs = model(images)
                loss = criterion(raw_outputs, targets)  # ä½¿ç”¨åŸå§‹è¾“å‡ºè®¡ç®—æŸå¤±
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                progress_bar.set_postfix({"train_loss": f"{loss.item():.4f}"})
            
            avg_train_loss = train_loss / len(train_loader)
            scheduler.step()  # æ¯è½®æ›´æ–°å­¦ä¹ ç‡

            # éªŒè¯é˜¶æ®µï¼ˆè®¡ç®—æŸå¤±+mAPï¼‰
            model.eval()
            val_loss = 0.0
            all_preds = []
            all_targets = []
            with torch.no_grad():
                val_bar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [éªŒè¯]")
                for images, targets in val_bar:
                    images = images.to(DEVICE)
                    targets = [t.to(DEVICE) for t in targets]
                    
                    processed_outputs, raw_outputs = model(images)
                    loss = criterion(raw_outputs, targets)
                    val_loss += loss.item()
                    
                    # æ”¶é›†é¢„æµ‹å’ŒçœŸå®ç›®æ ‡ï¼ˆç”¨äºè®¡ç®—mAPï¼‰
                    for i in range(processed_outputs.shape[0]):
                        pred_boxes = processed_outputs[i].cpu().numpy()
                        pred_boxes = pred_boxes[pred_boxes[:, 4] >= CONF_THRESHOLD]
                        pred_boxes = nms_boxes(pred_boxes, IOU_THRESHOLD)
                        all_preds.append(pred_boxes)
                        
                        target_boxes = targets[i].cpu().numpy()
                        all_targets.append(target_boxes)
                    
                    val_bar.set_postfix({"val_loss": f"{loss.item():.4f}"})
            
            avg_val_loss = val_loss / len(val_loader)
            # è®¡ç®—mAPï¼ˆç®€åŒ–ç‰ˆï¼Œå®Œæ•´å¯å‚è€ƒCOCOè¯„ä¼°æ ‡å‡†ï¼‰
            current_map = self.calculate_simple_map(all_preds, all_targets)

            # æ‰“å°è½®æ¬¡ä¿¡æ¯
            print(f"\nEpoch {epoch+1}/{EPOCHS}")
            print(f"  è®­ç»ƒæŸå¤±ï¼š{avg_train_loss:.4f} | éªŒè¯æŸå¤±ï¼š{avg_val_loss:.4f}")
            print(f"  å½“å‰å­¦ä¹ ç‡ï¼š{optimizer.param_groups[0]['lr']:.6f} | éªŒè¯mAPï¼š{current_map:.4f}")

            # ä¿å­˜æ¨¡å‹
            # 1. æ¯25è½®ä¿å­˜ä¸€æ¬¡ä¸­é—´æƒé‡
            if (epoch + 1) % 25 == 0:
                torch.save(model.state_dict(), weights_dir / f"epoch_{epoch+1}.pt")
                print(f"  ä¸­é—´æƒé‡ä¿å­˜ï¼š{weights_dir / f'epoch_{epoch+1}.pt'}")
            
            # 2. ä¿å­˜mAPæœ€é«˜çš„æœ€ä½³æ¨¡å‹
            if current_map > best_map:
                best_map = current_map
                torch.save(model.state_dict(), weights_dir / "best.pt")
                print(f"  æœ€ä½³æ¨¡å‹æ›´æ–°ï¼š{weights_dir / 'best.pt'}ï¼ˆå½“å‰mAPï¼š{best_map:.4f}ï¼‰")
            
            # 3. ä¿å­˜æœ€æ–°æ¨¡å‹
            torch.save(model.state_dict(), weights_dir / "last.pt")

        print("\n" + "=" * 80)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ“ æœ€ä½³æƒé‡ï¼š{weights_dir / 'best.pt'}ï¼ˆæœ€ä½³mAPï¼š{best_map:.4f}ï¼‰")
        print(f"ğŸ“ æœ€æ–°æƒé‡ï¼š{weights_dir / 'last.pt'}")
        print("=" * 80)
        return model

    def collate_fn(self, batch):
        """å¤„ç†ä¸åŒæ ·æœ¬ç›®æ ‡æ•°é‡ä¸ä¸€è‡´çš„é—®é¢˜ï¼ˆDataLoaderå¿…éœ€ï¼‰"""
        images, targets = zip(*batch)
        images = torch.stack(images, dim=0)  # å›¾åƒå †å ï¼ˆbatch, 3, 832, 1472ï¼‰
        return images, targets  # ç›®æ ‡ä¿æŒåˆ—è¡¨æ ¼å¼ï¼ˆæ¯ä¸ªå…ƒç´ æ˜¯ä¸åŒé•¿åº¦çš„tensorï¼‰

    def calculate_simple_map(self, all_preds, all_targets, iou_threshold=0.5):
        """ç®€åŒ–ç‰ˆmAPè®¡ç®—ï¼ˆé€‚ç”¨äºå¿«é€Ÿè¯„ä¼°ï¼ŒéCOCOæ ‡å‡†ï¼‰"""
        if len(all_preds) == 0 or len(all_targets) == 0:
            return 0.0
        
        avg_ap = 0.0
        for cls in range(NUM_CLASSES):
            # æ”¶é›†å½“å‰ç±»åˆ«çš„æ‰€æœ‰é¢„æµ‹å’ŒçœŸå®ç›®æ ‡
            pred_boxes = []
            target_boxes = []
            for i in range(len(all_preds)):
                # é¢„æµ‹æ¡†ï¼šç­›é€‰å½“å‰ç±»åˆ«+æŒ‰ç½®ä¿¡åº¦é™åº
                preds = [p for p in all_preds[i] if int(p[5]) == cls]
                preds = sorted(preds, key=lambda x: x[4], reverse=True)
                pred_boxes.extend(preds)
                
                # çœŸå®ç›®æ ‡ï¼šç­›é€‰å½“å‰ç±»åˆ«
                targets = [t for t in all_targets[i] if int(t[4]) == cls]
                target_boxes.extend(targets)
            
            if len(pred_boxes) == 0 or len(target_boxes) == 0:
                ap = 0.0
                avg_ap += ap
                continue
            
            # è®¡ç®—TPï¼ˆçœŸæ­£ä¾‹ï¼‰ã€FPï¼ˆå‡æ­£ä¾‹ï¼‰
            tp = [0] * len(pred_boxes)
            target_used = [False] * len(target_boxes)
            
            for pred_idx, pred in enumerate(pred_boxes):
                pred_bbox = pred[:4]
                max_iou = 0.0
                best_target_idx = -1
                
                for target_idx, target in enumerate(target_boxes):
                    if target_used[target_idx]:
                        continue
                    target_bbox = target[:4]
                    iou = calculate_iou(pred_bbox, target_bbox)
                    if iou > max_iou:
                        max_iou = iou
                        best_target_idx = target_idx
                
                if max_iou >= iou_threshold and best_target_idx != -1:
                    tp[pred_idx] = 1
                    target_used[best_target_idx] = True
            
            # è®¡ç®—Precisionå’ŒRecall
            tp_cumsum = np.cumsum(tp)
            fp_cumsum = np.cumsum([1 - x for x in tp])
            precision = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-6)  # åŠ 1e-6é¿å…é™¤0
            recall = tp_cumsum / len(target_boxes)
            
            # è®¡ç®—APï¼ˆå¹³å‡ç²¾åº¦ï¼‰ï¼šæ¢¯å½¢ç§¯åˆ†
            ap = 0.0
            prev_recall = 0.0
            for p, r in zip(precision, recall):
                if r > prev_recall:
                    ap += p * (r - prev_recall)
                    prev_recall = r
            
            avg_ap += ap
        
        # å¹³å‡APï¼ˆmAPï¼‰= æ‰€æœ‰ç±»åˆ«çš„APå¹³å‡å€¼
        return avg_ap / NUM_CLASSES

    def predict_image(self, model, image_path):
        """å•å›¾æ¨ç†ï¼ˆè¾“å‡ºYOLOæ ¼å¼ç»“æœï¼‰"""
        # å›¾åƒé¢„å¤„ç†
        image = cv2.imread(str(image_path))
        if image is None:
            print(f"âŒ æ— æ³•åŠ è½½å›¾åƒï¼š{image_path}")
            return [], image
        
        h_origin, w_origin = image.shape[:2]
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image_resized = cv2.resize(image_rgb, (IMG_SIZE[1], IMG_SIZE[0]))
        
        # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        input_tensor = transform(image_resized).unsqueeze(0).to(DEVICE)
        
        # æ¨ç†
        model.eval()
        with torch.no_grad():
            processed_outputs, _ = model(input_tensor)  # ä½¿ç”¨åå¤„ç†è¾“å‡ºç”¨äºæ¨ç†
            outputs = processed_outputs[0].cpu().numpy()
        
        # åå¤„ç†ï¼šè¿‡æ»¤ä½ç½®ä¿¡åº¦+NMS+åæ ‡è¿˜åŸ
        valid_boxes = outputs[outputs[:, 4] >= CONF_THRESHOLD]
        valid_boxes = nms_boxes(valid_boxes, IOU_THRESHOLD)
        
        # è¿˜åŸåˆ°åŸå§‹å›¾åƒå°ºå¯¸
        scale_x = w_origin / IMG_SIZE[1]
        scale_y = h_origin / IMG_SIZE[0]
        final_boxes = []
        for box in valid_boxes:
            x1 = box[0] * scale_x
            y1 = box[1] * scale_y
            x2 = box[2] * scale_x
            y2 = box[3] * scale_y
            conf = box[4]
            cls_id = int(box[5])
            
            # ä¿®æ­£åæ ‡ï¼ˆé¿å…è¶…å‡ºå›¾åƒèŒƒå›´ï¼‰
            x1 = max(0, min(x1, w_origin))
            y1 = max(0, min(y1, h_origin))
            x2 = max(0, min(x2, w_origin))
            y2 = max(0, min(y2, h_origin))
            
            final_boxes.append([x1, y1, x2, y2, conf, cls_id])
        
        return final_boxes, image

    def export_predictions(self, model_path):
        """å¯¼å‡ºéªŒè¯é›†é¢„æµ‹ç»“æœï¼ˆä¾›vote_config.pyä½¿ç”¨ï¼‰"""
        # åŠ è½½æ¨¡å‹
        model = ViTDetector().to(DEVICE)
        model.load_state_dict(torch.load(model_path, map_location=DEVICE))
        print(f"\nâœ… åŠ è½½æ¨¡å‹ï¼š{model_path}")
        
        # å‡†å¤‡è¾“å‡ºç›®å½•
        pred_dir = Path("./Pred")
        pred_labels_dir = pred_dir / "labels"
        pred_images_dir = pred_dir / "images"
        pred_labels_dir.mkdir(parents=True, exist_ok=True)
        pred_images_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½éªŒè¯é›†å›¾åƒ
        image_paths = []
        for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
            image_paths.extend(list(Path(VAL_IMAGES_PATH).glob(f'*{ext}')))
            image_paths.extend(list(Path(VAL_IMAGES_PATH).glob(f'*{ext.upper()}')))
        print(f"ğŸ“Š æ‰¾åˆ°éªŒè¯é›†å›¾åƒï¼š{len(image_paths)}å¼ ")
        
        # æ‰¹é‡æ¨ç†å¹¶ä¿å­˜ç»“æœ
        total_boxes = 0
        progress_bar = tqdm(image_paths, desc="å¯¼å‡ºé¢„æµ‹ç»“æœ")
        for img_path in progress_bar:
            img_name = img_path.name
            # æ¨ç†
            boxes, image = self.predict_image(model, img_path)
            if len(boxes) == 0:
                continue
            
            # ä¿å­˜å›¾åƒ
            shutil.copy2(str(img_path), str(pred_images_dir / img_name))
            
            # ä¿å­˜æ ‡ç­¾ï¼ˆYOLOæ ¼å¼ï¼šcls_id cx cy bw bh confï¼‰
            label_name = img_path.stem + '.txt'
            label_path = pred_labels_dir / label_name
            h_origin, w_origin = image.shape[:2]
            
            with open(label_path, 'w') as f:
                for box in boxes:
                    x1, y1, x2, y2, conf, cls_id = box
                    # è½¬æ¢ä¸ºYOLOç›¸å¯¹åæ ‡
                    cx = ((x1 + x2) / 2) / w_origin
                    cy = ((y1 + y2) / 2) / h_origin
                    bw = (x2 - x1) / w_origin
                    bh = (y2 - y1) / h_origin
                    # å†™å…¥æ ¼å¼ï¼šcls_id cx cy bw bh conf
                    f.write(f"{cls_id} {cx:.6f} {cy:.6f} {bw:.6f} {bh:.6f} {conf:.6f}\n")
                    total_boxes += 1
        
        # åˆ›å»ºdata.yaml
        data_yaml_path = pred_dir / "data.yaml"
        data_config = {
            'train': '',
            'val': str(VAL_IMAGES_PATH),
            'test': '',
            'nc': NUM_CLASSES,
            'names': self.class_names,
            'val_labels': str(VAL_LABELS_PATH)
        }
        with open(data_yaml_path, 'w') as f:
            yaml.dump(data_config, f, default_flow_style=False, allow_unicode=True)
        
        print(f"ğŸ‰ é¢„æµ‹ç»“æœå¯¼å‡ºå®Œæˆï¼š{pred_dir}")
        print(f"   å…±ç”Ÿæˆé¢„æµ‹æ¡†ï¼š{total_boxes}ä¸ª | æ•°æ®é…ç½®ï¼š{data_yaml_path}")
        return pred_dir


# ============================== ä¸»å‡½æ•°ï¼ˆä¸€é”®è¿è¡Œï¼‰==============================
def main():
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = ViTDetectorTrainer()
    
    # è®­ç»ƒæ¨¡å‹
    trainer.train_model()
    
    # å¯¼å‡ºé¢„æµ‹ç»“æœï¼ˆè‹¥æœ€ä½³æ¨¡å‹å­˜åœ¨ï¼‰
    if Path(BEST_MODEL_PATH).exists():
        trainer.export_predictions(BEST_MODEL_PATH)
    else:
        print(f"âš ï¸ æœ€ä½³æ¨¡å‹ä¸å­˜åœ¨ï¼š{BEST_MODEL_PATH}ï¼Œè·³è¿‡é¢„æµ‹å¯¼å‡º")


if __name__ == '__main__':
    main()