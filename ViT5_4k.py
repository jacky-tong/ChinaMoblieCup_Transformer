import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision.models import vit_b_16
from pathlib import Path
import cv2
import numpy as np
from tqdm import tqdm
import psutil
from PIL import Image

# ================= æ ¸å¿ƒé…ç½®ï¼ˆé‡ç‚¹ï¼ï¼‰=================
DATASET_ROOT = r"E:\2025CompetitionLog\9.9\tRVALCUP_2\ChinaMoblieCup\dataset" 
TRAIN_OUTPUT_DIR = "./runs/vit_detector"
MODEL_NAME = "vit_detector"
BEST_MODEL_PATH = os.path.join(TRAIN_OUTPUT_DIR, MODEL_NAME, "weights", "best.pt")

# ç¦»çº¿æƒé‡è·¯å¾„ï¼ˆå¿…é¡»æ­£ç¡®ï¼‰
LOCAL_VIT_WEIGHTS_PATH = r"E:\transformer\model--clip--vit\vit_b_16-c867db91.pth"

# ViTå…³é”®å‚æ•°ï¼ˆå’Œé¢„è®­ç»ƒå¯¹é½ï¼‰
IMG_SIZE = (224, 224)
VIT_MEAN = [0.485, 0.456, 0.406]  # ViTé¢„è®­ç»ƒç”¨çš„å‡å€¼
VIT_STD = [0.229, 0.224, 0.225]   # ViTé¢„è®­ç»ƒç”¨çš„æ ‡å‡†å·®
NUM_CLASSES = 4

# è®­ç»ƒè¶…å‚æ•°ï¼ˆé’ˆå¯¹ViTä¼˜åŒ–ï¼‰
EPOCHS = 200  # å…ˆè·‘50è½®çœ‹æ”¶æ•›
BATCH_SIZE = 8
GRADIENT_ACCUMULATION_STEPS = 2  # å‡å°‘ç´¯ç§¯ï¼ŒåŠ å¿«åé¦ˆ
BASE_LR = 1e-5  # é™ä½å­¦ä¹ ç‡ï¼Œä¿æŠ¤é¢„è®­ç»ƒæƒé‡
WEIGHT_DECAY = 1e-4
WARMUP_EPOCHS = 3  # å»¶é•¿warmupï¼Œå¹³ç¨³è¿‡æ¸¡
FREEZE_LAYERS = 4  # å‡å°‘å†»ç»“å±‚æ•°ï¼Œä¿ç•™æ›´å¤šå¯è®­ç»ƒå‚æ•°
DROPOUT_RATE = 0.1

# å°ç›®æ ‡é…ç½®
SMALL_OBJ_AREA_THR = 0.01  # é¢ç§¯å æ¯”<1%ä¸ºå°ç›®æ ‡
SMALL_OBJ_WEIGHT = 1.5     # å°ç›®æ ‡æŸå¤±æƒé‡

# è®¾å¤‡é…ç½®ï¼ˆä¸»è¿›ç¨‹æ‰“å°ä¸€æ¬¡ï¼‰
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
if os.getpid() == 0 or psutil.Process().parent().pid == 0:  # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°
    print(f"âœ… ä¸»è¿›ç¨‹ä½¿ç”¨è®¾å¤‡: {DEVICE}")

# ================= æ•°æ®é›†ï¼ˆä¿®å¤é¢„å¤„ç†+æ ‡ç­¾å½’ä¸€åŒ–ï¼‰=================
class CustomDetectionDataset(Dataset):
    def __init__(self, img_dir, label_dir, img_size=IMG_SIZE, is_train=True):
        self.img_dir = Path(img_dir)
        self.label_dir = Path(label_dir)
        self.img_size = img_size
        self.is_train = is_train
        
        # ä¿®å¤ï¼šViTä¸“ç”¨é¢„å¤„ç†ï¼ˆå¯¹é½é¢„è®­ç»ƒåˆ†å¸ƒï¼‰
        self.transform = self._get_transforms()
        
        # åŠ è½½å›¾åƒåˆ—è¡¨ï¼ˆåªåœ¨ä¸»è¿›ç¨‹æ‰“å°æ•°é‡ï¼‰
        self.images = list(self.img_dir.glob("*.jpg")) + list(self.img_dir.glob("*.png")) + \
                     list(self.img_dir.glob("*.JPG")) + list(self.img_dir.glob("*.PNG"))
        if os.getpid() == 0 or psutil.Process().parent().pid == 0:
            print(f"âœ… æ‰¾åˆ° {len(self.images)} å¼ å›¾åƒ")

    def _get_transforms(self):
        """ä¿®å¤ï¼šViTä¸“ç”¨æ•°æ®å¢å¼º+å½’ä¸€åŒ–ï¼Œä¿è¯è¾“å‡ºå›ºå®šå°ºå¯¸"""
        # è¾“å‡ºå°ºå¯¸ï¼ˆPIL transforms æ¥å— (H, W)ï¼‰
        out_size = (self.img_size[0], self.img_size[1])
        
        if self.is_train:
            # è®­ç»ƒï¼šéšæœºå¤šå°ºåº¦è£å‰ªï¼Œè¾“å‡ºå›ºå®šå°ºå¯¸
            aug_list = [
                transforms.RandomResizedCrop(size=out_size, scale=(0.8, 1.2)),
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
            ]
        else:
            # éªŒè¯/æµ‹è¯•ï¼šå…ˆç¼©æ”¾çŸ­è¾¹å†ä¸­å¿ƒè£å‰ªï¼Œä¿è¯è¾“å‡ºå›ºå®šå°ºå¯¸
            aug_list = [
                transforms.Resize(out_size),
                transforms.CenterCrop(out_size),
            ]

        # æœ€åè½¬æ¢ä¸º Tensor å¹¶å¯¹é½ ViT é¢„è®­ç»ƒåˆ†å¸ƒ
        aug_list += [
            transforms.ToTensor(),
            transforms.Normalize(mean=VIT_MEAN, std=VIT_STD)
        ]
        return transforms.Compose(aug_list)

    def _small_obj_crop(self, img, targets):
        """å°ç›®æ ‡ä¼˜å…ˆè£å‰ªï¼ˆå¢å¼ºå°ç›®æ ‡ç‰¹å¾ï¼‰"""
        h, w = img.shape[:2]
        total_area = h * w
        small_targets = [t for t in targets if (t[2]-t[0])*(t[3]-t[1])/total_area < SMALL_OBJ_AREA_THR]
        if not small_targets:
            return img, targets
        
        # éšæœºé€‰ä¸€ä¸ªå°ç›®æ ‡è£å‰ª
        target = small_targets[np.random.randint(len(small_targets))]
        x1, y1, x2, y2 = target[:4]
        cx, cy = (x1+x2)/2, (y1+y2)/2
        crop_w = max(x2-x1, w*0.3)  # æœ€å°è£å‰ª30%å®½åº¦
        crop_h = max(y2-y1, h*0.3)
        crop_x1 = max(0, cx - crop_w/2)
        crop_y1 = max(0, cy - crop_h/2)
        crop_x2 = min(w, cx + crop_w/2)
        crop_y2 = min(h, cy + crop_h/2)
        
        # è£å‰ªå›¾åƒ+è°ƒæ•´ç›®æ ‡åæ ‡
        img_cropped = img[int(crop_y1):int(crop_y2), int(crop_x1):int(crop_x2)]
        new_targets = []
        for t in targets:
            tx1, ty1, tx2, ty2, tcls = t
            ntx1 = max(0, tx1 - crop_x1)
            nty1 = max(0, ty1 - crop_y1)
            ntx2 = min(crop_x2 - crop_x1, tx2 - crop_x1)
            nty2 = min(crop_y2 - crop_y1, ty2 - crop_y1)
            if ntx1 < ntx2 and nty1 < nty2:
                new_targets.append([ntx1, nty1, ntx2, nty2, tcls])
        
        # æ¢å¤åŸå°ºå¯¸+è°ƒæ•´åæ ‡å°ºåº¦
        img_resized = cv2.resize(img_cropped, (w, h))
        scale_x = w / (crop_x2 - crop_x1)
        scale_y = h / (crop_y2 - crop_y1)
        for t in new_targets:
            t[0] *= scale_x
            t[1] *= scale_y
            t[2] *= scale_x
            t[3] *= scale_y
        
        return img_resized, new_targets if new_targets else [[0,0,0,0,-1]]

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path = self.images[idx]
        label_path = self.label_dir / (img_path.stem + ".txt")

        # åŠ è½½å›¾åƒ
        img = cv2.imread(str(img_path))
        if img is None:
            img = np.zeros((self.img_size[0], self.img_size[1], 3), dtype=np.uint8)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # è½¬RGBï¼ˆå’ŒViTé¢„è®­ç»ƒå¯¹é½ï¼‰
        h, w = img.shape[:2]

        # åŠ è½½æ ‡ç­¾ï¼ˆä¿®å¤ï¼šå½’ä¸€åŒ–åˆ°0-1ï¼‰
        targets = []
        if label_path.exists():
            with open(label_path, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) == 5:
                        cls_id, cx, cy, bw, bh = map(float, parts)
                        # ä¿®å¤ï¼šä»ç›¸å¯¹åæ ‡â†’ç»å¯¹åæ ‡â†’å½’ä¸€åŒ–åˆ°0-1
                        x1 = (cx - bw/2) * w / self.img_size[1]  # é™¤ä»¥img_sizeå½’ä¸€åŒ–
                        y1 = (cy - bh/2) * h / self.img_size[0]
                        x2 = (cx + bw/2) * w / self.img_size[1]
                        y2 = (cy + bh/2) * h / self.img_size[0]
                        #  clampé˜²æ­¢è¶…ç•Œ
                        x1, y1, x2, y2 = max(0, x1), max(0, y1), min(1, x2), min(1, y2)
                        targets.append([x1, y1, x2, y2, cls_id])
        
        # å°ç›®æ ‡å¢å¼ºï¼ˆè®­ç»ƒé˜¶æ®µï¼‰
        if self.is_train and np.random.random() < 0.5:
            img, targets = self._small_obj_crop(img, targets)
        
        # é¢„å¤„ç†å›¾åƒï¼šç¡®ä¿ä¼ å…¥ torchvision.transforms çš„ä¸º PIL.Image æˆ– Tensor
        if isinstance(img, np.ndarray):
            # ç¡®ä¿ uint8 ç±»å‹ä»¥å…¼å®¹ PIL
            img = img.astype('uint8')
            img_pil = Image.fromarray(img)
        else:
            img_pil = img
        img_tensor = self.transform(img_pil)
        
        # å¤„ç†ç©ºæ ‡ç­¾
        if not targets:
            targets = [[0, 0, 0, 0, -1]]
        
        return img_tensor, torch.tensor(targets, dtype=torch.float32)

def collate_fn(batch):
    """ä¿®å¤ï¼šé¿å…ç›®æ ‡å¡«å……å¯¼è‡´çš„ç»´åº¦æ··ä¹±"""
    images, targets = zip(*batch)
    images = torch.stack(images)
    max_targets = max(len(t) for t in targets) if targets else 1
    
    padded_targets = []
    for t in targets:
        if len(t) < max_targets:
            pad = torch.full((max_targets - len(t), 5), -1, dtype=torch.float32)
            t_padded = torch.cat([t, pad])
        else:
            t_padded = t[:max_targets]  # é˜²æ­¢ä¸ªåˆ«æ ·æœ¬ç›®æ ‡è¿‡å¤š
        padded_targets.append(t_padded)
    
    return images, torch.stack(padded_targets)

# ================= æ¨¡å‹ï¼ˆä¿®å¤å†»ç»“å±‚æ•°+è¾“å…¥å¯¹é½ï¼‰=================
class DetectionHead(nn.Module):
    def __init__(self, embed_dim, num_classes, dropout_rate=DROPOUT_RATE):
        super().__init__()
        self.bbox_head = nn.Sequential(
            nn.Linear(embed_dim, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 4),
            nn.Sigmoid()  # è¾“å‡º0-1ï¼Œå’Œæ ‡ç­¾å¯¹é½
        )
        self.cls_head = nn.Sequential(
            nn.Linear(embed_dim, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, num_classes + 1)  # +1èƒŒæ™¯
        )
        self.conf_head = nn.Sequential(
            nn.Linear(embed_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        bbox_pred = self.bbox_head(x)
        cls_pred = self.cls_head(x)
        conf_pred = self.conf_head(x)
        return torch.cat([bbox_pred, conf_pred, cls_pred], dim=-1)

class ViTDetector(nn.Module):
    def __init__(self, freeze_layers=FREEZE_LAYERS, num_classes=NUM_CLASSES):
        super().__init__()
        
        # ç¬¬ä¸€æ­¥ï¼šå…ˆå®šä¹‰ img_sizeï¼ˆå…³é”®ï¼é¿å…åç»­å¼•ç”¨æ—¶æœªå®šä¹‰ï¼‰
        self.img_size = IMG_SIZE  # ä»å…¨å±€é…ç½®è·å–å›¾åƒå°ºå¯¸
        
        # ç¦»çº¿åŠ è½½ViTï¼ˆä¿®å¤ï¼šweights_only=Trueé¿å…è­¦å‘Šï¼‰
        if LOCAL_VIT_WEIGHTS_PATH and os.path.isfile(LOCAL_VIT_WEIGHTS_PATH):
            print(f"ğŸ“¥ åŠ è½½æœ¬åœ°ViTæƒé‡: {LOCAL_VIT_WEIGHTS_PATH}")
            self.backbone = vit_b_16(weights=None)
            # æ·»åŠ weights_only=Trueé¿å…FutureWarning
            state_dict = torch.load(LOCAL_VIT_WEIGHTS_PATH, map_location="cpu", weights_only=True)
            self.backbone.load_state_dict(state_dict)
        else:
            print("âš ï¸ æœ¬åœ°æƒé‡ä¸å­˜åœ¨ï¼Œåœ¨çº¿ä¸‹è½½ViT")
            self.backbone = vit_b_16(weights="IMAGENET1K_V1")
        
        # ç¬¬äºŒæ­¥ï¼šè®¡ç®—num_patchesï¼ˆæ­¤æ—¶self.img_sizeå·²å®šä¹‰ï¼‰
        ps = self.backbone.patch_size  # patch_sizeæ˜¯æ•´æ•°ï¼ˆå¦‚16ï¼‰
        self.embed_dim = self.backbone.hidden_dim
        self.num_patches = (self.img_size[0] // ps) * (self.img_size[1] // ps)
        
        # å†»ç»“æŒ‡å®šå±‚æ•°
        self._freeze_layers(freeze_layers)
        
        # æ£€æµ‹å¤´
        self.detection_head = DetectionHead(self.embed_dim, num_classes)

    def _freeze_layers(self, freeze_layers):
        """å†»ç»“patchåµŒå…¥+å‰Nå±‚Transformer"""
        # å†»ç»“patchåµŒå…¥å±‚
        for param in self.backbone.conv_proj.parameters():
            param.requires_grad = False
        
        # å†»ç»“å‰Nå±‚Transformer
        for i, layer in enumerate(self.backbone.encoder.layers):
            if i < freeze_layers:
                for param in layer.parameters():
                    param.requires_grad = False
            else:
                for param in layer.parameters():
                    param.requires_grad = True
        print(f"âœ… å†»ç»“å‰ {freeze_layers} å±‚Transformerï¼Œå‰©ä½™ {12-freeze_layers} å±‚å¯è®­ç»ƒ")

    def forward(self, x):
        # ViTå‰å‘ä¼ æ’­ï¼ˆä¸¥æ ¼å¯¹é½å®˜æ–¹å®ç°ï¼‰
        x = self.backbone._process_input(x)
        n = x.shape[0]
        
        # æ·»åŠ class tokenå’Œä½ç½®ç¼–ç 
        batch_class_token = self.backbone.class_token.expand(n, -1, -1)
        x = torch.cat([batch_class_token, x], dim=1)
        x = x + self.backbone.encoder.pos_embedding
        
        # Transformerç¼–ç 
        x = self.backbone.encoder(x)
        
        # å–patch tokenï¼ˆæ’é™¤class tokenï¼‰
        patch_tokens = x[:, 1:]  # [B, num_patches, embed_dim]
        
        # æ£€æµ‹å¤´é¢„æµ‹
        return self.detection_head(patch_tokens)

# ================= æŸå¤±å‡½æ•°ï¼ˆä¿®å¤å°ºåº¦åŒ¹é…+å°ç›®æ ‡åŠ æƒï¼‰=================
class DetectionLoss(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.num_classes = num_classes
        self.bbox_loss = nn.SmoothL1Loss(reduction='none')  # é€å…ƒç´ è®¡ç®—ï¼Œæ–¹ä¾¿åŠ æƒ
        self.cls_loss = nn.CrossEntropyLoss(reduction='none')

    def forward(self, preds, targets):
        """
        preds: [B, N_patches, 5 + num_classes] â†’ [bbox(4), conf(1), cls(N+1)]
        targets: [B, max_objects, 5] â†’ [x1,y1,x2,y2,cls_id]ï¼ˆ0-1å½’ä¸€åŒ–ï¼Œ-1ä¸ºå¡«å……ï¼‰
        """
        B, N_patches, _ = preds.shape
        total_loss = 0.0

        for b in range(B):
            pred = preds[b]  # [N_patches, 5+C]
            target = targets[b]  # [max_objects, 5]
            target = target[target[:, 4] != -1]  # è¿‡æ»¤å¡«å……æ ‡ç­¾
            
            # æƒ…å†µ1ï¼šæ— æœ‰æ•ˆç›®æ ‡ï¼ˆåªç®—èƒŒæ™¯ç½®ä¿¡åº¦æŸå¤±ï¼‰
            if target.size(0) == 0:
                conf_loss = F.binary_cross_entropy(pred[:, 4], torch.zeros(N_patches, device=pred.device))
                total_loss += conf_loss
                continue

            # æƒ…å†µ2ï¼šæœ‰æœ‰æ•ˆç›®æ ‡ï¼ˆè®¡ç®—åŒ¹é…æŸå¤±ï¼‰
            pred_boxes = pred[:, :4]  # [N_patches, 4]
            target_boxes = target[:, :4]  # [N_targets, 4]
            
            # è®¡ç®—IoUï¼ˆä¿®å¤ï¼šç¡®ä¿ç»´åº¦æ­£ç¡®ï¼‰
            ious = self._calc_iou(pred_boxes.unsqueeze(1), target_boxes.unsqueeze(0))  # [N_patches, N_targets]
            best_ious, best_target_idx = ious.max(dim=1)  # æ¯ä¸ªpatchåŒ¹é…æœ€ä½³ç›®æ ‡

            # ä¿®å¤ï¼šå°ç›®æ ‡åŒ¹é…é˜ˆå€¼é™ä½ï¼ˆ0.4ï¼‰ï¼Œæé«˜å°ç›®æ ‡å¬å›
            matched_mask = best_ious > 0.4
            if matched_mask.sum() == 0:  # æ— åŒ¹é…æ—¶ï¼Œå–IoUæœ€å¤§çš„patch
                matched_mask = best_ious == best_ious.max()

            # 1. è¾¹ç•Œæ¡†æŸå¤±ï¼ˆä¿®å¤ï¼šå°ç›®æ ‡åŠ æƒï¼‰
            matched_pred_boxes = pred_boxes[matched_mask]
            matched_target_boxes = target_boxes[best_target_idx[matched_mask]]
            
            # å°ç›®æ ‡åŠ æƒï¼šé¢ç§¯è¶Šå°ï¼Œæƒé‡è¶Šé«˜
            target_areas = (matched_target_boxes[:, 2] - matched_target_boxes[:, 0]) * \
                           (matched_target_boxes[:, 3] - matched_target_boxes[:, 1])
            bbox_weights = torch.where(target_areas < SMALL_OBJ_AREA_THR, 
                                      torch.tensor(SMALL_OBJ_WEIGHT, device=pred.device),
                                      torch.tensor(1.0, device=pred.device))
            bbox_loss = (self.bbox_loss(matched_pred_boxes, matched_target_boxes) * 
                         bbox_weights.unsqueeze(1)).mean()

            # 2. åˆ†ç±»æŸå¤±
            matched_pred_cls = pred[matched_mask, 6:]  # è·³è¿‡bbox(4)+conf(1)+èƒŒæ™¯(1)
            matched_target_cls = target[best_target_idx[matched_mask], 4].long()
            cls_loss = self.cls_loss(matched_pred_cls, matched_target_cls).mean()

            # 3. ç½®ä¿¡åº¦æŸå¤±ï¼ˆåŒ¹é…ç›®æ ‡â†’1ï¼ŒæœªåŒ¹é…â†’0ï¼‰
            matched_conf_loss = F.binary_cross_entropy(pred[matched_mask, 4], 
                                                      torch.ones(matched_mask.sum(), device=pred.device))
            unmatched_mask = ~matched_mask
            if unmatched_mask.sum() > 0:
                unmatched_conf_loss = F.binary_cross_entropy(pred[unmatched_mask, 4], 
                                                            torch.zeros(unmatched_mask.sum(), device=pred.device))
                conf_loss = (matched_conf_loss + unmatched_conf_loss) / 2
            else:
                conf_loss = matched_conf_loss

            # æ€»æŸå¤±ï¼ˆæƒé‡å¹³è¡¡ï¼‰
            total_loss += bbox_loss * 5.0 + cls_loss * 1.0 + conf_loss * 2.0  # bboxæŸå¤±æƒé‡æ›´é«˜

        return total_loss / B  # å¹³å‡åˆ°æ¯ä¸ªbatch

    def _calc_iou(self, boxes1, boxes2):
        """è®¡ç®—IoUï¼ˆboxes1: [N,1,4], boxes2: [1,M,4]ï¼‰"""
        x1 = torch.max(boxes1[:, :, 0], boxes2[:, :, 0])
        y1 = torch.max(boxes1[:, :, 1], boxes2[:, :, 1])
        x2 = torch.min(boxes1[:, :, 2], boxes2[:, :, 2])
        y2 = torch.min(boxes1[:, :, 3], boxes2[:, :, 3])
        
        intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)
        area1 = (boxes1[:, :, 2] - boxes1[:, :, 0]) * (boxes1[:, :, 3] - boxes1[:, :, 1])
        area2 = (boxes2[:, :, 2] - boxes2[:, :, 0]) * (boxes2[:, :, 3] - boxes2[:, :, 1])
        
        union = area1 + area2 - intersection
        return intersection / (union + 1e-6)  # é¿å…é™¤ä»¥0

# ================= è®­ç»ƒå™¨ï¼ˆä¿®å¤å­¦ä¹ ç‡+æ—¥å¿—æ¸…æ™°ï¼‰=================
class Trainer:
    def __init__(self):
        self.device = DEVICE
        self._setup_dirs()

    def _setup_dirs(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        os.makedirs(os.path.join(TRAIN_OUTPUT_DIR, MODEL_NAME, "weights"), exist_ok=True)
        print(f"âœ… è¾“å‡ºç›®å½•: {TRAIN_OUTPUT_DIR}")

    def setup_data(self):
        """ä¿®å¤ï¼šDataLoaderå‚æ•°ä¼˜åŒ–ï¼ˆé¿å…å¤šworkeré‡å¤æ‰“å°ï¼‰"""
        # è®­ç»ƒé›†
        train_dataset = CustomDetectionDataset(
            img_dir=os.path.join(DATASET_ROOT, "images", "train"),
            label_dir=os.path.join(DATASET_ROOT, "labels", "train"),
            is_train=True
        )
        # éªŒè¯é›†
        val_dataset = CustomDetectionDataset(
            img_dir=os.path.join(DATASET_ROOT, "images", "val"),
            label_dir=os.path.join(DATASET_ROOT, "labels", "val"),
            is_train=False
        )
        
        # ä¼˜åŒ–ï¼šnum_workers=4ï¼ˆé¿å…è¿‡å¤šå¯¼è‡´èµ„æºå ç”¨ï¼‰ï¼Œpin_memory=TrueåŠ é€Ÿ
        num_workers = min(psutil.cpu_count() // 4, 4)
        train_loader = DataLoader(
            train_dataset, batch_size=BATCH_SIZE, shuffle=True,
            num_workers=num_workers, collate_fn=collate_fn, pin_memory=True,
            persistent_workers=True  # å¤ç”¨workerï¼Œå‡å°‘é‡å¤åˆå§‹åŒ–
        )
        val_loader = DataLoader(
            val_dataset, batch_size=BATCH_SIZE*2, shuffle=False,  # éªŒè¯é›†batchåŠ å€
            num_workers=num_workers, collate_fn=collate_fn, pin_memory=True,
            persistent_workers=True
        )
        
        # æ¸…æ™°æ‰“å°æ•°æ®ä¿¡æ¯
        print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"  - è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾, {len(train_loader)} æ‰¹æ¬¡")
        print(f"  - éªŒè¯é›†: {len(val_dataset)} å¼ å›¾, {len(val_loader)} æ‰¹æ¬¡")
        print(f"  - æ•°æ®worker: {num_workers} ä¸ª")
        return train_loader, val_loader

    def setup_model(self):
        """ä¿®å¤ï¼šå­¦ä¹ ç‡è°ƒåº¦+æ¨¡å‹å‚æ•°ç»Ÿè®¡"""
        # åˆå§‹åŒ–æ¨¡å‹
        model = ViTDetector().to(self.device)
        
        # ä¼˜åŒ–å™¨ï¼ˆåªè®­ç»ƒå¯å­¦ä¹ å‚æ•°ï¼‰
        trainable_params = filter(lambda p: p.requires_grad, model.parameters())
        optimizer = torch.optim.AdamW(
            trainable_params, lr=BASE_LR, weight_decay=WEIGHT_DECAY,
            betas=(0.9, 0.999)  # ç¨³å®šä¼˜åŒ–
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆä¿®å¤ï¼šwarmupåä½™å¼¦é€€ç«ï¼‰
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=EPOCHS - WARMUP_EPOCHS, eta_min=BASE_LR * 0.01  # æœ€å°å­¦ä¹ ç‡
        )
        
        # æŸå¤±å‡½æ•°
        criterion = DetectionLoss(NUM_CLASSES).to(self.device)
        
        # ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°
        trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_num = sum(p.numel() for p in model.parameters())
        print(f"ğŸ“ˆ æ¨¡å‹å‚æ•°: æ€»å‚æ•° {total_num//1000}K, å¯è®­ç»ƒå‚æ•° {trainable_num//1000}K")
        
        return model, optimizer, scheduler, criterion

    def _get_current_lr(self, optimizer, epoch):
        """ä¿®å¤ï¼šwarmupå­¦ä¹ ç‡ï¼ˆä»1e-7çº¿æ€§å‡åˆ°BASE_LRï¼‰"""
        if epoch < WARMUP_EPOCHS:
            return BASE_LR * (epoch + 1) / WARMUP_EPOCHS + 1e-7
        return optimizer.param_groups[0]['lr']

    def train_epoch(self, model, train_loader, optimizer, criterion, epoch):
        """å•è½®è®­ç»ƒï¼ˆä¿®å¤ï¼šæ¢¯åº¦ç´¯ç§¯+æ¸…æ™°æ—¥å¿—ï¼‰"""
        model.train()
        total_loss = 0.0
        current_lr = self._get_current_lr(optimizer, epoch)
        
        # è®¾ç½®å½“å‰å­¦ä¹ ç‡
        for param_group in optimizer.param_groups:
            param_group['lr'] = current_lr
        
        # è¿›åº¦æ¡ï¼ˆæ˜¾ç¤ºå­¦ä¹ ç‡ï¼‰
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} | LR: {current_lr:.2e} | Train")
        
        for step, (images, targets) in enumerate(pbar):
            images = images.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)
            
            # å‰å‘ä¼ æ’­
            preds = model(images)
            loss = criterion(preds, targets)
            
            # æ¢¯åº¦ç´¯ç§¯
            loss = loss / GRADIENT_ACCUMULATION_STEPS
            loss.backward()
            
            # ç´¯ç§¯åˆ°æŒ‡å®šæ­¥æ•°åæ›´æ–°
            if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:
                optimizer.step()
                optimizer.zero_grad()
            
            # ç»Ÿè®¡æŸå¤±
            total_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'Step Loss': f'{loss.item()*GRADIENT_ACCUMULATION_STEPS:.4f}',
                'Avg Loss': f'{total_loss/(step+1):.4f}'
            })
        
        # å¤„ç†å‰©ä½™æ¢¯åº¦
        if (step + 1) % GRADIENT_ACCUMULATION_STEPS != 0:
            optimizer.step()
            optimizer.zero_grad()
        
        return total_loss / len(train_loader)

    def val_epoch(self, model, val_loader, criterion, epoch):
        """å•è½®éªŒè¯ï¼ˆæ— æ¢¯åº¦è®¡ç®—ï¼‰"""
        model.eval()
        total_loss = 0.0
        pbar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{EPOCHS} | Val")
        
        with torch.no_grad():
            for images, targets in pbar:
                images = images.to(self.device, non_blocking=True)
                targets = targets.to(self.device, non_blocking=True)
                
                preds = model(images)
                loss = criterion(preds, targets)
                total_loss += loss.item()
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'Val Step Loss': f'{loss.item():.4f}',
                    'Val Avg Loss': f'{total_loss/(pbar.n//BATCH_SIZE + 1):.4f}'
                })
        
        return total_loss / len(val_loader)

    def train(self):
        """ä¸»è®­ç»ƒæµç¨‹"""
        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹è®­ç»ƒViTç›®æ ‡æ£€æµ‹å™¨ï¼ˆå°ç›®æ ‡ä¼˜åŒ–ç‰ˆï¼‰")
        print("="*60 + "\n")
        
        # åŠ è½½æ•°æ®å’Œæ¨¡å‹
        train_loader, val_loader = self.setup_data()
        model, optimizer, scheduler, criterion = self.setup_model()
        
        # è®­ç»ƒè®°å½•
        best_val_loss = float('inf')
        
        for epoch in range(EPOCHS):
            print(f"\nğŸ“… Epoch {epoch+1}/{EPOCHS}")
            print("-"*50)
            
            # è®­ç»ƒ
            train_loss = self.train_epoch(model, train_loader, optimizer, criterion, epoch)
            
            # éªŒè¯
            val_loss = self.val_epoch(model, val_loader, criterion, epoch)
            
            # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆwarmupåç”Ÿæ•ˆï¼‰
            if epoch >= WARMUP_EPOCHS:
                scheduler.step()
            
            # æ‰“å°ç»“æœ
            print(f"ğŸ“Š Epoch {epoch+1} ç»“æœ:")
            print(f"  - è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"  - éªŒè¯æŸå¤±: {val_loss:.4f}")
            print(f"  - å½“å‰æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save({
                    'epoch': epoch+1,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'best_val_loss': best_val_loss
                }, BEST_MODEL_PATH)
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {BEST_MODEL_PATH}")
            
            # æ¯10è½®ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % 10 == 0:
                checkpoint_path = os.path.join(
                    TRAIN_OUTPUT_DIR, MODEL_NAME, "weights", f"checkpoint_epoch_{epoch+1}.pt"
                )
                torch.save(model.state_dict(), checkpoint_path)
                print(f"ğŸ“ ä¿å­˜æ£€æŸ¥ç‚¹åˆ°: {checkpoint_path}")
        
        # è®­ç»ƒå®Œæˆ
        print("\n" + "="*60)
        print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        print(f"ğŸ“Œ æœ€ä½³æ¨¡å‹è·¯å¾„: {BEST_MODEL_PATH}")
        print("="*60)

if __name__ == "__main__":
    # æ£€æŸ¥æ•°æ®é›†è·¯å¾„
    if not os.path.exists(DATASET_ROOT):
        print(f"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {DATASET_ROOT}")
        exit()
    # æ£€æŸ¥æƒé‡è·¯å¾„
    if not os.path.exists(LOCAL_VIT_WEIGHTS_PATH):
        print(f"âŒ ViTæƒé‡è·¯å¾„ä¸å­˜åœ¨: {LOCAL_VIT_WEIGHTS_PATH}")
        exit()
    # å¼€å§‹è®­ç»ƒ
    trainer = Trainer()
    trainer.train()