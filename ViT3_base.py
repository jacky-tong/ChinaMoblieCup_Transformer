import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision.models import vit_b_16, ViT_B_16_Weights
from pathlib import Path
import cv2
import numpy as np
from tqdm import tqdm
from torch.amp import autocast, GradScaler
import psutil
import math
from einops import rearrange
from contextlib import contextmanager

# ================= é…ç½® =================
DATASET_ROOT = r"E:\2025CompetitionLog\9.9\tRVALCUP_2\ChinaMoblieCup\dataset" 
TRAIN_OUTPUT_DIR = "./runs/vit_detector"
MODEL_NAME = "vit_detector"
BEST_MODEL_PATH = os.path.join(TRAIN_OUTPUT_DIR, MODEL_NAME, "weights", "best.pt")

IMG_SIZE = (224, 224)  # ViT-Baseçš„æ ‡å‡†è¾“å…¥å°ºå¯¸
PATCH_SIZE = 16
NUM_CLASSES = 4

# è®­ç»ƒè¶…å‚æ•°ï¼ˆä¼˜åŒ–åçš„é…ç½®ï¼‰
EPOCHS = 100
BATCH_SIZE = 8  # å¢åŠ æ‰¹å¤„ç†å¤§å°
GRADIENT_ACCUMULATION_STEPS = 4  # å‡å°‘æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
BASE_LR = 2e-5
WEIGHT_DECAY = 1e-4
WARMUP_EPOCHS = 2
FREEZE_LAYERS = 8  # ViT-Baseæœ‰12å±‚ï¼Œå†»ç»“å‰8å±‚

DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"âœ… ä½¿ç”¨è®¾å¤‡: {DEVICE}")

# å…¼å®¹ä¸åŒ PyTorch ç‰ˆæœ¬çš„ GradScaler/ autocast å°è£…
def create_grad_scaler(device):
    """åˆ›å»ºå…¼å®¹ä¸åŒç‰ˆæœ¬çš„ GradScalerï¼›å¦‚æœéCUDAè¿”å›None"""
    if device.type != 'cuda':
        return None
    # ä¼˜å…ˆå°è¯•æ— å‚æ•°æ„é€ ï¼ˆå¸¸è§ç‰ˆæœ¬ï¼‰ï¼Œå›é€€åˆ° device_type å‚æ•°ï¼ˆéƒ¨åˆ†ç‰ˆæœ¬ï¼‰
    try:
        return GradScaler()
    except TypeError:
        try:
            return GradScaler(device_type='cuda')
        except TypeError:
            # æœ€åå…œåº•å†æ¬¡å°è¯•æ— å‚æ•°æ„é€ 
            return GradScaler()

@contextmanager
def autocast_context(enabled):
    """å…¼å®¹ä¸åŒç‰ˆæœ¬ autocast çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆenabled: æ˜¯å¦å¯ç”¨ï¼‰"""
    if not enabled:
        yield
        return
    try:
        # æ–°ç‰ˆ torch.amp.autocast å¯èƒ½éœ€è¦ device_type å‚æ•°
        with autocast(device_type='cuda'):
            yield
    except TypeError:
        # ä½ç‰ˆæœ¬å¯èƒ½ä¸æ¥å— device_type
        with autocast():
            yield

# ================= æ•°æ®é›† =================
class CustomDetectionDataset(Dataset):
    def __init__(self, img_dir, label_dir, img_size=IMG_SIZE, transform=None):
        self.img_dir = Path(img_dir)
        self.label_dir = Path(label_dir)
        self.img_size = img_size
        
        # æ•°æ®å¢å¼º
        if transform is None:
            self.transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        else:
            self.transform = transform
            
        # è·å–å›¾åƒæ–‡ä»¶åˆ—è¡¨
        self.images = list(self.img_dir.glob("*.jpg")) + list(self.img_dir.glob("*.png")) + \
                     list(self.img_dir.glob("*.JPG")) + list(self.img_dir.glob("*.PNG"))
        
        print(f"âœ… æ‰¾åˆ° {len(self.images)} å¼ å›¾åƒ")

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path = self.images[idx]
        label_path = self.label_dir / (img_path.stem + ".txt")

        # åŠ è½½å›¾åƒ
        img = cv2.imread(str(img_path))
        if img is None:
            print(f"âš ï¸ æ— æ³•è¯»å–å›¾åƒ: {img_path}")
            # è¿”å›é»‘è‰²å›¾åƒä½œä¸ºå ä½ç¬¦
            img = np.zeros((self.img_size[0], self.img_size[1], 3), dtype=np.uint8)
        else:
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        
        # è°ƒæ•´å°ºå¯¸
        h, w = img.shape[:2]
        img_resized = cv2.resize(img, (self.img_size[1], self.img_size[0]))
        
        # åº”ç”¨å˜æ¢
        img_tensor = self.transform(img_resized)

        # å¤„ç†æ ‡ç­¾
        targets = []
        if label_path.exists():
            with open(label_path, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) == 5:  # ç¡®ä¿æœ‰5ä¸ªå€¼
                        cls_id, cx, cy, bw, bh = map(float, parts)
                        # è½¬æ¢ä¸ºç»å¯¹åæ ‡
                        x1 = max(0, (cx - bw/2) * self.img_size[1])
                        y1 = max(0, (cy - bh/2) * self.img_size[0])
                        x2 = min(self.img_size[1], (cx + bw/2) * self.img_size[1])
                        y2 = min(self.img_size[0], (cy + bh/2) * self.img_size[0])
                        
                        targets.append([x1, y1, x2, y2, cls_id])
        
        # å¦‚æœæ²¡æœ‰ç›®æ ‡ï¼Œæ·»åŠ ä¸€ä¸ªç©ºç›®æ ‡
        if len(targets) == 0:
            targets.append([0, 0, 0, 0, -1])  # ä½¿ç”¨-1è¡¨ç¤ºæ— ç›®æ ‡

        return img_tensor, torch.tensor(targets, dtype=torch.float32)

def collate_fn(batch):
    """è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°"""
    images, targets = zip(*batch)
    images = torch.stack(images)
    
    # æ‰¾åˆ°æ‰¹æ¬¡ä¸­æœ€å¤šçš„ç›®æ ‡æ•°
    max_targets = max(len(t) for t in targets)
    
    # å¡«å……ç›®æ ‡å¼ é‡
    padded_targets = []
    for t in targets:
        if len(t) < max_targets:
            # ä½¿ç”¨-1å¡«å……
            pad = torch.full((max_targets - len(t), 5), -1, dtype=torch.float32)
            t_padded = torch.cat([t, pad])
        else:
            t_padded = t
        padded_targets.append(t_padded)
    
    return images, torch.stack(padded_targets)

# ================= æ¨¡å‹å®šä¹‰ =================
class DetectionHead(nn.Module):
    """ç›®æ ‡æ£€æµ‹å¤´ - é¢„æµ‹è¾¹ç•Œæ¡†å’Œç±»åˆ«"""
    def __init__(self, embed_dim, num_classes, num_patches):
        super().__init__()
        self.num_patches = num_patches
        self.embed_dim = embed_dim
        
        # è¾¹ç•Œæ¡†é¢„æµ‹ (x1, y1, x2, y2)
        self.bbox_head = nn.Sequential(
            nn.Linear(embed_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 4),
            nn.Sigmoid()  # è¾“å‡ºåœ¨0-1èŒƒå›´å†…
        )
        
        # ç±»åˆ«é¢„æµ‹
        self.cls_head = nn.Sequential(
            nn.Linear(embed_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, num_classes + 1)  # +1 for background/no object
        )
        
        # ç›®æ ‡ç½®ä¿¡åº¦
        self.conf_head = nn.Sequential(
            nn.Linear(embed_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        """x shape: (B, num_patches, embed_dim)"""
        B, N, D = x.shape
        
        # ä¸ºæ¯ä¸ªpatché¢„æµ‹è¾¹ç•Œæ¡†ã€ç±»åˆ«å’Œç½®ä¿¡åº¦
        bbox_pred = self.bbox_head(x)  # (B, N, 4)
        cls_pred = self.cls_head(x)    # (B, N, num_classes+1)
        conf_pred = self.conf_head(x)  # (B, N, 1)
        
        # åˆå¹¶æ‰€æœ‰é¢„æµ‹
        outputs = torch.cat([bbox_pred, conf_pred, cls_pred], dim=-1)  # (B, N, 5 + num_classes)
        return outputs

class ViTDetector(nn.Module):
    """åŸºäºViTçš„ç›®æ ‡æ£€æµ‹å™¨"""
    def __init__(self, freeze_layers=FREEZE_LAYERS, num_classes=NUM_CLASSES):
        super().__init__()
        
        # åŠ è½½é¢„è®­ç»ƒçš„ViT-Baseæ¨¡å‹
        weights = ViT_B_16_Weights.DEFAULT
        self.backbone = vit_b_16(weights=weights)
        
        # è·å–æ¨¡å‹å‚æ•°
        self.patch_size = self.backbone.patch_size  # ç›´æ¥ä½¿ç”¨æ•´æ•°ï¼Œä¸ä¸‹æ ‡è®¿é—®
        self.embed_dim = self.backbone.hidden_dim
        
        # è®¡ç®—patchæ•°é‡
        self.num_patches = (IMG_SIZE[0] // self.patch_size) * (IMG_SIZE[1] // self.patch_size)
        
        # å†»ç»“æŒ‡å®šå±‚æ•°
        self._freeze_layers(freeze_layers)
        
        # æ£€æµ‹å¤´ - ä¸ºæ¯ä¸ªpatché¢„æµ‹ç›®æ ‡
        self.detection_head = DetectionHead(self.embed_dim, num_classes, self.num_patches)

    def _freeze_layers(self, freeze_layers):
        """å†»ç»“æŒ‡å®šå±‚æ•°"""
        # å†»ç»“patchåµŒå…¥
        for param in self.backbone.conv_proj.parameters():
            param.requires_grad = False
            
        # å†»ç»“Transformerå±‚
        for i, layer in enumerate(self.backbone.encoder.layers):
            if i < freeze_layers:
                for param in layer.parameters():
                    param.requires_grad = False
            else:
                for param in layer.parameters():
                    param.requires_grad = True
        
        print(f"âœ… å†»ç»“äº†å‰ {freeze_layers} å±‚Transformer")

    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        B, C, H, W = x.shape
        
        # ViTå‰å‘ä¼ æ’­
        x = self.backbone._process_input(x)  # Patch embedding
        n = x.shape[0]
        
        # æ‰©å±•class tokenå’Œä½ç½®ç¼–ç 
        batch_class_token = self.backbone.class_token.expand(n, -1, -1)
        x = torch.cat([batch_class_token, x], dim=1)
        x = x + self.backbone.encoder.pos_embedding
        
        # Transformerç¼–ç å™¨
        x = self.backbone.encoder(x)
        
        # ä½¿ç”¨æ‰€æœ‰patch tokensï¼ˆæ’é™¤class tokenï¼‰è¿›è¡Œæ£€æµ‹
        patch_tokens = x[:, 1:]  # (B, num_patches, embed_dim)
        
        # æ£€æµ‹å¤´
        outputs = self.detection_head(patch_tokens)
        
        return outputs

# ================= æŸå¤±å‡½æ•° =================
class DetectionLoss(nn.Module):
    """æ”¹è¿›çš„ç›®æ ‡æ£€æµ‹æŸå¤±å‡½æ•°"""
    def __init__(self, num_classes, num_patches):
        super().__init__()
        self.num_classes = num_classes
        self.num_patches = num_patches
        
        # æŸå¤±å‡½æ•°
        self.bbox_loss = nn.SmoothL1Loss(reduction='none')
        self.cls_loss = nn.CrossEntropyLoss(reduction='none')
        self.conf_loss = nn.BCEWithLogitsLoss(reduction='none')

    def forward(self, preds, targets):
        """
        preds: (B, num_patches, 5 + num_classes)
        targets: (B, max_targets, 5) - æœ€åä¸€ç»´æ˜¯[x1, y1, x2, y2, class_id]
        """
        B, N, _ = preds.shape
        total_loss = 0
        
        for i in range(B):
            # å½“å‰æ‰¹æ¬¡çš„é¢„æµ‹å’Œç›®æ ‡
            pred = preds[i]  # (N, 5 + num_classes)
            target = targets[i]  # (max_targets, 5)
            
            # è¿‡æ»¤æœ‰æ•ˆç›®æ ‡ï¼ˆclass_id >= 0ï¼‰
            valid_targets = target[target[:, 4] >= 0]
            if len(valid_targets) == 0:
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆç›®æ ‡ï¼Œåªè®¡ç®—èƒŒæ™¯æŸå¤±
                conf_loss = F.binary_cross_entropy_with_logits(
                    pred[:, 4], torch.zeros(N, device=pred.device), reduction='mean'
                )
                total_loss += conf_loss
                continue
            
            # ç®€åŒ–çš„åŒ¹é…ç­–ç•¥ï¼šé€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„patchä½œä¸ºé¢„æµ‹
            # åœ¨å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„åŒ¹é…ç­–ç•¥å¦‚åŒˆç‰™åˆ©ç®—æ³•
            with torch.no_grad():
                # è®¡ç®—æ¯ä¸ªpatchä¸æ¯ä¸ªç›®æ ‡çš„IoU
                pred_boxes = pred[:, :4]
                target_boxes = valid_targets[:, :4]
                
                # è®¡ç®—IoU
                ious = self._calculate_iou(pred_boxes.unsqueeze(1), target_boxes.unsqueeze(0))
                best_ious, best_target_idx = ious.max(dim=1)
                
                # é€‰æ‹©ä¸ç›®æ ‡æœ‰æœ€å¤§IoUçš„patch
                matched = best_ious > 0.5
                if matched.sum() == 0:
                    # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œä½¿ç”¨æœ€é«˜IoUçš„patch
                    matched = best_ious == best_ious.max()
            
            # è®¡ç®—åŒ¹é…ç›®æ ‡çš„æŸå¤±
            if matched.sum() > 0:
                matched_pred = pred[matched]
                matched_target_idx = best_target_idx[matched]
                matched_targets = valid_targets[matched_target_idx]
                
                # è¾¹ç•Œæ¡†æŸå¤±
                bbox_loss = self.bbox_loss(matched_pred[:, :4], matched_targets[:, :4]).mean()
                
                # ç±»åˆ«æŸå¤±
                cls_loss = self.cls_loss(
                    matched_pred[:, 6:],  # è·³è¿‡bbox(4)å’Œconf(1)ã€èƒŒæ™¯ç±»(1)
                    matched_targets[:, 4].long()
                ).mean()
                
                # ç½®ä¿¡åº¦æŸå¤±ï¼ˆåŒ¹é…çš„ç›®æ ‡ç½®ä¿¡åº¦åº”ä¸º1ï¼‰
                conf_loss = F.binary_cross_entropy_with_logits(
                    matched_pred[:, 4], torch.ones(matched.sum(), device=pred.device)
                )
                
                # æœªåŒ¹é…patchçš„ç½®ä¿¡åº¦æŸå¤±ï¼ˆåº”ä¸º0ï¼‰
                unmatched = ~matched
                if unmatched.sum() > 0:
                    unmatched_conf_loss = F.binary_cross_entropy_with_logits(
                        pred[unmatched, 4], torch.zeros(unmatched.sum(), device=pred.device)
                    )
                    conf_loss = (conf_loss + unmatched_conf_loss) / 2
                
                total_loss += bbox_loss + cls_loss + conf_loss
        
        return total_loss / B

    def _calculate_iou(self, boxes1, boxes2):
        """è®¡ç®—IoU"""
        # boxes1: (N, 1, 4), boxes2: (1, M, 4)
        x1 = torch.max(boxes1[:, :, 0], boxes2[:, :, 0])
        y1 = torch.max(boxes1[:, :, 1], boxes2[:, :, 1])
        x2 = torch.min(boxes1[:, :, 2], boxes2[:, :, 2])
        y2 = torch.min(boxes1[:, :, 3], boxes2[:, :, 3])
        
        intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)
        area1 = (boxes1[:, :, 2] - boxes1[:, :, 0]) * (boxes1[:, :, 3] - boxes1[:, :, 1])
        area2 = (boxes2[:, :, 2] - boxes2[:, :, 0]) * (boxes2[:, :, 3] - boxes2[:, :, 1])
        
        union = area1 + area2 - intersection
        iou = intersection / (union + 1e-6)
        
        return iou

# ================= è®­ç»ƒå™¨ç±» =================
class ViTTrainer:
    def __init__(self):
        self.device = DEVICE
        self.setup_directories()
        
    def setup_directories(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        os.makedirs(os.path.join(TRAIN_OUTPUT_DIR, MODEL_NAME, "weights"), exist_ok=True)
        print(f"âœ… è¾“å‡ºç›®å½•åˆ›å»ºå®Œæˆ: {TRAIN_OUTPUT_DIR}")

    def setup_data_loaders(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        # æ•°æ®å¢å¼º
        train_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        val_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # æ•°æ®é›†
        train_dataset = CustomDetectionDataset(
            img_dir=os.path.join(DATASET_ROOT, "images", "train"),
            label_dir=os.path.join(DATASET_ROOT, "labels", "train"),
            transform=train_transform
        )
        
        val_dataset = CustomDetectionDataset(
            img_dir=os.path.join(DATASET_ROOT, "images", "val"),
            label_dir=os.path.join(DATASET_ROOT, "labels", "val"),
            transform=val_transform
        )
        
        # æ•°æ®åŠ è½½å™¨
        num_workers = min(psutil.cpu_count() // 2, 8)
        
        train_loader = DataLoader(
            train_dataset, batch_size=BATCH_SIZE, shuffle=True,
            num_workers=num_workers, collate_fn=collate_fn, pin_memory=True
        )
        
        val_loader = DataLoader(
            val_dataset, batch_size=BATCH_SIZE, shuffle=False,
            num_workers=num_workers, collate_fn=collate_fn, pin_memory=True
        )
        
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
        print(f"  è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾åƒ, {len(train_loader)} ä¸ªæ‰¹æ¬¡")
        print(f"  éªŒè¯é›†: {len(val_dataset)} å¼ å›¾åƒ, {len(val_loader)} ä¸ªæ‰¹æ¬¡")
        
        return train_loader, val_loader

    def setup_model_and_optimizer(self):
        """è®¾ç½®æ¨¡å‹å’Œä¼˜åŒ–å™¨"""
        model = ViTDetector().to(self.device)
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            filter(lambda p: p.requires_grad, model.parameters()),
            lr=BASE_LR, weight_decay=WEIGHT_DECAY
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)
        
        # æŸå¤±å‡½æ•°
        num_patches = (IMG_SIZE[0] // PATCH_SIZE) * (IMG_SIZE[1] // PATCH_SIZE)
        criterion = DetectionLoss(NUM_CLASSES, num_patches).to(self.device)
        
        # æ¢¯åº¦ç¼©æ”¾ï¼ˆæ··åˆç²¾åº¦è®­ç»ƒï¼‰ - ä½¿ç”¨å…¼å®¹åˆ›å»ºå‡½æ•°
        scaler = create_grad_scaler(self.device)
        
        return model, optimizer, scheduler, criterion, scaler

    def train_epoch(self, model, train_loader, optimizer, criterion, scaler, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        model.train()
        train_loss = 0
        accumulation_steps = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [è®­ç»ƒ]")
        
        for i, (images, targets) in enumerate(progress_bar):
            images = images.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)
            
            # æ··åˆç²¾åº¦è®­ç»ƒ - ä½¿ç”¨å…¼å®¹ autocast ä¸Šä¸‹æ–‡
            if scaler is not None:
                with autocast_context(True):
                    preds = model(images)
                    loss = criterion(preds, targets) / GRADIENT_ACCUMULATION_STEPS
                
                # æ¢¯åº¦ç´¯ç§¯
                scaler.scale(loss).backward()
                accumulation_steps += 1
                
                if accumulation_steps % GRADIENT_ACCUMULATION_STEPS == 0:
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()
            else:
                # CPUè®­ç»ƒ
                preds = model(images)
                loss = criterion(preds, targets) / GRADIENT_ACCUMULATION_STEPS
                loss.backward()
                accumulation_steps += 1
                
                if accumulation_steps % GRADIENT_ACCUMULATION_STEPS == 0:
                    optimizer.step()
                    optimizer.zero_grad()
            
            train_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'Loss': f'{loss.item() * GRADIENT_ACCUMULATION_STEPS:.4f}',
                'Avg Loss': f'{train_loss / (i + 1):.4f}'
            })
        
        # å¤„ç†å‰©ä½™çš„æ¢¯åº¦
        if accumulation_steps % GRADIENT_ACCUMULATION_STEPS != 0:
            if scaler is not None:
                scaler.step(optimizer)
                scaler.update()
            else:
                optimizer.step()
            optimizer.zero_grad()
        
        return train_loss / len(train_loader)

    def validate_epoch(self, model, val_loader, criterion, epoch):
        """éªŒè¯ä¸€ä¸ªepoch"""
        model.eval()
        val_loss = 0
        
        progress_bar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [éªŒè¯]")
        
        with torch.no_grad():
            for images, targets in progress_bar:
                images = images.to(self.device, non_blocking=True)
                targets = targets.to(self.device, non_blocking=True)
                
                # æ··åˆç²¾åº¦éªŒè¯ - ä½¿ç”¨å…¼å®¹ autocast ä¸Šä¸‹æ–‡
                if self.device.type == 'cuda':
                    with autocast_context(True):
                        preds = model(images)
                        loss = criterion(preds, targets)
                else:
                    preds = model(images)
                    loss = criterion(preds, targets)
                
                val_loss += loss.item()
                
                progress_bar.set_postfix({
                    'Val Loss': f'{loss.item():.4f}',
                    'Avg Val Loss': f'{val_loss / (progress_bar.n + 1):.4f}'
                })
        
        return val_loss / len(val_loader)

    def train_model(self):
        """ä¸»è®­ç»ƒå‡½æ•°"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒViTç›®æ ‡æ£€æµ‹å™¨...")
        
        # è®¾ç½®æ•°æ®åŠ è½½å™¨
        train_loader, val_loader = self.setup_data_loaders()
        
        # è®¾ç½®æ¨¡å‹å’Œä¼˜åŒ–å™¨
        model, optimizer, scheduler, criterion, scaler = self.setup_model_and_optimizer()
        
        best_val_loss = float('inf')
        train_losses = []
        val_losses = []
        
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
        
        for epoch in range(EPOCHS):
            print(f"\n{'='*50}")
            print(f"ğŸ“… Epoch {epoch+1}/{EPOCHS}")
            print(f"{'='*50}")
            
            # è®­ç»ƒ
            train_loss = self.train_epoch(model, train_loader, optimizer, criterion, scaler, epoch)
            train_losses.append(train_loss)
            
            # éªŒè¯
            val_loss = self.validate_epoch(model, val_loader, criterion, epoch)
            val_losses.append(val_loss)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step()
            
            print(f"ğŸ“ˆ è®­ç»ƒæŸå¤±: {train_loss:.4f} | éªŒè¯æŸå¤±: {val_loss:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'train_loss': train_loss,
                    'val_loss': val_loss,
                    'best_val_loss': best_val_loss
                }, BEST_MODEL_PATH)
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {BEST_MODEL_PATH}")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % 10 == 0:
                checkpoint_path = os.path.join(
                    TRAIN_OUTPUT_DIR, MODEL_NAME, "weights", f"checkpoint_epoch_{epoch+1}.pt"
                )
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'train_loss': train_loss,
                    'val_loss': val_loss,
                }, checkpoint_path)
                print(f"ğŸ“ ä¿å­˜æ£€æŸ¥ç‚¹åˆ°: {checkpoint_path}")
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        
        # ä¿å­˜è®­ç»ƒè®°å½•
        training_log = {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'best_val_loss': best_val_loss,
            'epochs': EPOCHS
        }
        
        log_path = os.path.join(TRAIN_OUTPUT_DIR, MODEL_NAME, "training_log.npy")
        np.save(log_path, training_log)
        print(f"ğŸ“Š è®­ç»ƒè®°å½•ä¿å­˜åˆ°: {log_path}")

# ================= ä¸»å‡½æ•° =================
def main():
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
    if not os.path.exists(DATASET_ROOT):
        print(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {DATASET_ROOT}")
        return
    
    print("ğŸ” æ£€æŸ¥æ•°æ®é›†ç»“æ„...")
    train_img_dir = os.path.join(DATASET_ROOT, "images", "train")
    train_label_dir = os.path.join(DATASET_ROOT, "labels", "train")
    
    if not os.path.exists(train_img_dir) or not os.path.exists(train_label_dir):
        print("âŒ æ•°æ®é›†ç»“æ„ä¸æ­£ç¡®ï¼Œè¯·ç¡®ä¿å­˜åœ¨ images/train å’Œ labels/train ç›®å½•")
        return
    
    # å¼€å§‹è®­ç»ƒ
    trainer = ViTTrainer()
    trainer.train_model()

if __name__ == "__main__":
    main()