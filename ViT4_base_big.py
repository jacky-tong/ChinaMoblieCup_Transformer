import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision.models import vit_l_16, ViT_L_16_Weights
from pathlib import Path
import cv2
import numpy as np
from tqdm import tqdm
from torch.amp import autocast, GradScaler
import psutil

# ============================ 1. å…¨å±€é…ç½® ============================
DATASET_ROOT = r"/root/lowTrain/dataset"
TRAIN_OUTPUT_DIR = "./runs/vit_large_detector_832x1472"
MODEL_NAME = "vit_large_832x1472_detector"
BEST_MODEL_PATH = os.path.join(TRAIN_OUTPUT_DIR, MODEL_NAME, "weights", "best.pt")
LAST_MODEL_PATH = os.path.join(TRAIN_OUTPUT_DIR, MODEL_NAME, "weights", "last.pt")

IMG_SIZE = (832, 1472)
PATCH_SIZE = 16
NUM_CLASSES = 4
EMBED_DIM = 1024
NUM_PATCHES = (IMG_SIZE[0] // PATCH_SIZE) * (IMG_SIZE[1] // PATCH_SIZE)  # 4784

EPOCHS = 100
BATCH_SIZE = 2
GRADIENT_ACCUMULATION_STEPS = 8
BASE_LR = 1e-5
WEIGHT_DECAY = 1e-4
FREEZE_LAYERS = 16
LOSS_LAMBDA_BOX = 5.0
LOSS_LAMBDA_CLS = 1.0
WARMUP_EPOCHS = 20  # å‰ 20 è½®ç”¨å•å‘åŒ¹é…ï¼Œä¹‹åç”¨åŒå‘åŒ¹é…

DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"âœ… ä½¿ç”¨è®¾å¤‡ï¼š{DEVICE}")


# ============================ 2. æ•°æ®é›† ============================
class CustomDetectionDataset(Dataset):
    def __init__(self, img_dir, label_dir, is_train=True):
        self.img_dir = Path(img_dir)
        self.label_dir = Path(label_dir)
        self.is_train = is_train
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        self.img_paths = []
        for ext in [".jpg", ".jpeg", ".png", ".bmp", ".JPG", ".JPEG"]:
            self.img_paths.extend(list(self.img_dir.glob(f"*{ext}")))
        if len(self.img_paths) == 0:
            raise FileNotFoundError(f"âŒ åœ¨ {img_dir} æœªæ‰¾åˆ°å›¾ç‰‡")
        print(f"ğŸ“Š åŠ è½½{len(self.img_paths)}å¼ {'è®­ç»ƒ' if is_train else 'éªŒè¯'}å›¾")

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):
        img_path = self.img_paths[idx]
        img = cv2.imread(str(img_path))
        if img is None:
            raise ValueError(f"âŒ æ— æ³•åŠ è½½å›¾åƒï¼š{img_path}")
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img_resized = cv2.resize(img_rgb, (IMG_SIZE[1], IMG_SIZE[0]))
        img_tensor = self.transform(img_resized)

        label_path = self.label_dir / (img_path.stem + ".txt")
        targets = []
        if label_path.exists():
            with open(label_path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line or len(line.split()) != 5:
                        continue
                    cls_id, cx_rel, cy_rel, bw_rel, bh_rel = map(float, line.split())
                    cx_abs = cx_rel * IMG_SIZE[1]
                    cy_abs = cy_rel * IMG_SIZE[0]
                    bw_abs = bw_rel * IMG_SIZE[1]
                    bh_abs = bh_rel * IMG_SIZE[0]
                    x1 = cx_abs - bw_abs / 2
                    y1 = cy_abs - bh_abs / 2
                    x2 = cx_abs + bw_abs / 2
                    y2 = cy_abs + bh_abs / 2
                    targets.append([x1, y1, x2, y2, cls_id])

        targets_tensor = torch.tensor(targets, dtype=torch.float32) if targets else torch.empty((0, 5), dtype=torch.float32)
        return img_tensor, targets_tensor

def collate_fn(batch):
    images, targets = zip(*batch)
    return torch.stack(images), list(targets)


# ============================ 3. æ¨¡å‹ ============================
class ViTLargeBackbone(nn.Module):
    def __init__(self, freeze_layers=FREEZE_LAYERS):
        super().__init__()
        weights = ViT_L_16_Weights.DEFAULT
        vit = vit_l_16(weights=weights)
        self.conv_proj = vit.conv_proj
        self.transformer_layers = vit.encoder.layers

        for param in self.conv_proj.parameters():
            param.requires_grad = False
        for layer in self.transformer_layers[:freeze_layers]:
            for param in layer.parameters():
                param.requires_grad = False

        self.pos_embedding = nn.Parameter(torch.randn(1, NUM_PATCHES, EMBED_DIM))
        nn.init.trunc_normal_(self.pos_embedding, std=0.02)

    def forward(self, x):
        x = self.conv_proj(x)
        x = x.flatten(2).transpose(1, 2)
        x = x + self.pos_embedding
        for layer in self.transformer_layers:
            x = layer(x)
        return x


class ViTDetector(nn.Module):
    def __init__(self):
        super().__init__()
        self.backbone = ViTLargeBackbone(freeze_layers=FREEZE_LAYERS)
        self.detection_head = nn.Sequential(
            nn.Linear(EMBED_DIM, 512),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(512, 5 + NUM_CLASSES)
        )
        self._init_detection_head()

    def _init_detection_head(self):
        for m in self.detection_head.modules():
            if isinstance(m, nn.Linear):
                nn.init.trunc_normal_(m.weight, std=0.02)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x):
        patch_features = self.backbone(x)
        raw_preds = self.detection_head(patch_features)
        batch_size = x.shape[0]
        processed_preds = []
        for i in range(batch_size):
            pred = raw_preds[i]
            bboxes = pred[:, :4].clone()
            x1 = bboxes[:, 0].clamp(0, IMG_SIZE[1])
            y1 = bboxes[:, 1].clamp(0, IMG_SIZE[0])
            x2 = bboxes[:, 2].clamp(0, IMG_SIZE[1])
            y2 = bboxes[:, 3].clamp(0, IMG_SIZE[0])
            conf = torch.sigmoid(pred[:, 4:5])
            cls_logits = pred[:, 5:]
            cls_probs = F.softmax(cls_logits, dim=-1)
            cls_scores, cls_ids = torch.max(cls_probs, dim=-1, keepdim=True)
            final_conf = conf * cls_scores
            pred_i = torch.cat([x1.unsqueeze(-1), y1.unsqueeze(-1), x2.unsqueeze(-1), y2.unsqueeze(-1),
                                final_conf, cls_ids.float()], dim=-1)
            processed_preds.append(pred_i)
        processed_preds = torch.stack(processed_preds, dim=0)
        return processed_preds, raw_preds


# ============================ 4. è‡ªé€‚åº”åŒ¹é…æŸå¤±å‡½æ•° ============================
class DetectionLoss(nn.Module):
    def __init__(self, warmup_epochs=WARMUP_EPOCHS):
        super().__init__()
        self.box_loss = nn.SmoothL1Loss(reduction="sum")
        self.cls_loss = nn.CrossEntropyLoss(reduction="sum")
        self.conf_loss = nn.BCEWithLogitsLoss(reduction="sum")
        self.warmup_epochs = warmup_epochs

    def calculate_iou(self, box1, box2):
        x1_max = max(box1[0], box2[0])
        y1_max = max(box1[1], box2[1])
        x2_min = min(box1[2], box2[2])
        y2_min = min(box1[3], box2[3])
        if x2_min <= x1_max or y2_min <= y1_max:
            return 0.0
        intersection = (x2_min - x1_max) * (y2_min - y1_max)
        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
        union = area1 + area2 - intersection
        return intersection / union if union > 0 else 0.0

    def forward(self, raw_preds, targets, epoch):
        total_loss = 0.0
        batch_size = raw_preds.shape[0]

        for i in range(batch_size):
            pred = raw_preds[i]  # shape: (num_preds, 5 + num_classes)
            target = targets[i]  # shape: (num_targets, 5) or empty

            if target.numel() == 0:
                # æ²¡æœ‰ç›®æ ‡æ—¶ï¼Œåªè®¡ç®—ç½®ä¿¡åº¦ä¸º 0 çš„æŸå¤±
                conf_loss = self.conf_loss(pred[:, 4], torch.zeros(pred.shape[0], device=pred.device))
                total_loss += conf_loss
                continue

            num_preds = pred.shape[0]
            num_targets = target.shape[0]

            # è®¡ç®— IoU çŸ©é˜µï¼ˆnum_preds, num_targetsï¼‰
            iou_matrix = torch.zeros((num_preds, num_targets), device=pred.device)
            # ä½¿ç”¨ CPU numpy è®¡ç®— IoUï¼ˆä¿æŒåŸé£æ ¼ï¼‰ï¼Œä½†åªè½¬æ¢å¿…è¦çš„å°å¼ é‡
            pred_boxes_cpu = pred[:, :4].detach().cpu().numpy()
            target_boxes_cpu = target[:, :4].detach().cpu().numpy()
            for p_idx in range(num_preds):
                for t_idx in range(num_targets):
                    iou_matrix[p_idx, t_idx] = self.calculate_iou(pred_boxes_cpu[p_idx], target_boxes_cpu[t_idx])

            # åŒ¹é…é€»è¾‘ï¼šè¿”å›æˆå¯¹ç´¢å¼• listsï¼Œä¿è¯ pred ä¸ target ä¸€ä¸€å¯¹åº”ï¼ˆç›¸åŒé•¿åº¦ï¼‰
            matched_pred_indices = []
            matched_target_indices = []

            if epoch < self.warmup_epochs:
                # å•å‘æœ€å¤§åŒ¹é…ï¼šæ¯ä¸ª target é€‰å– IoU æœ€å¤§çš„ predï¼ˆå¯èƒ½å­˜åœ¨å¤šä¸ª target æŒ‡å‘åŒä¸€ä¸ª predï¼‰
                matched_pred_ids = torch.argmax(iou_matrix, dim=0)  # len = num_targets
                # matched_pred_ids[t] æ˜¯å¯¹åº” target t çš„ pred ç´¢å¼•
                matched_pred_indices = matched_pred_ids.tolist()
                matched_target_indices = list(range(num_targets))
            else:
                # åŒå‘æœ€å¤§åŒ¹é…ï¼šè¦æ±‚äº’ä¸ºæœ€ä¼˜
                matched_pred_ids = torch.argmax(iou_matrix, dim=0)  # target -> pred
                matched_target_ids = torch.argmax(iou_matrix, dim=1)  # pred -> target
                for t_idx in range(num_targets):
                    p_idx = int(matched_pred_ids[t_idx].item())
                    if int(matched_target_ids[p_idx].item()) == t_idx:
                        matched_pred_indices.append(p_idx)
                        matched_target_indices.append(t_idx)

                if len(matched_pred_indices) != num_targets:
                    # ä»…ä½œæç¤ºï¼Œå®é™…ä½¿ç”¨çš„æ˜¯äº’ä¸ºæœ€ä¼˜çš„å¯¹å­
                    print(f"è­¦å‘Šï¼šæ ·æœ¬ {i} äº’åŒ¹é…å¯¹æ•° {len(matched_pred_indices)} ä¸ç›®æ ‡æ•° {num_targets} ä¸ä¸€è‡´")

            # å¦‚æœæ²¡æœ‰ä»»ä½•åŒ¹é…å¯¹ï¼Œè·³è¿‡ box/cls æŸå¤±ï¼ˆåªè®¡ç®—ç½®ä¿¡åº¦æŸå¤±ï¼‰
            if len(matched_pred_indices) == 0:
                conf_target = torch.zeros(num_preds, device=pred.device)
                conf_loss = self.conf_loss(pred[:, 4], conf_target)
                total_loss += conf_loss
                continue

            # æŒ‰é…å¯¹ç´¢å¼•å–å‡ºå¯¹åº”çš„é¢„æµ‹å’Œç›®æ ‡ï¼ˆä¿è¯å½¢çŠ¶ä¸€è‡´ï¼‰
            pred_pos_boxes = pred[matched_pred_indices, :4]
            target_pos_boxes = target[matched_target_indices, :4].to(pred.device)

            # æ¡†æŸå¤±
            box_loss = self.box_loss(pred_pos_boxes, target_pos_boxes)

            # ç±»åˆ«æŸå¤±ï¼šCrossEntropy requires (N, C) and (N,)
            pred_pos_cls = pred[matched_pred_indices, 5:].to(pred.device)  # (M, num_classes)
            target_pos_cls = target[matched_target_indices, 4].long().to(pred.device)  # (M,)

            cls_loss = self.cls_loss(pred_pos_cls, target_pos_cls)

            # ç½®ä¿¡åº¦æŸå¤±ï¼šæ­£æ ·æœ¬ä¸º 1ï¼Œå…¶å®ƒä¸º 0
            conf_target = torch.zeros(num_preds, device=pred.device)
            for p_idx in matched_pred_indices:
                conf_target[p_idx] = 1.0
            conf_loss = self.conf_loss(pred[:, 4], conf_target)

            total_loss += LOSS_LAMBDA_BOX * box_loss + LOSS_LAMBDA_CLS * cls_loss + conf_loss

        return total_loss / batch_size


# ============================ 5. è®­ç»ƒå‡½æ•° ============================
def train():
    os.makedirs(os.path.join(TRAIN_OUTPUT_DIR, MODEL_NAME, "weights"), exist_ok=True)

    train_dataset = CustomDetectionDataset(
        img_dir=os.path.join(DATASET_ROOT, "images", "train"),
        label_dir=os.path.join(DATASET_ROOT, "labels", "train"),
        is_train=True
    )
    val_dataset = CustomDetectionDataset(
        img_dir=os.path.join(DATASET_ROOT, "images", "val"),
        label_dir=os.path.join(DATASET_ROOT, "labels", "val"),
        is_train=False
    )

    num_workers = min(psutil.cpu_count() // 2, 8)
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True,
        drop_last=True
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True
    )

    model = ViTDetector().to(DEVICE)
    print(f"âœ… ViT-Large/16æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ˆå‚æ•°æ€»æ•°ï¼š{sum(p.numel() for p in model.parameters())/1e6:.1f}Mï¼‰")

    optimizer = torch.optim.AdamW([
        {"params": model.backbone.transformer_layers[FREEZE_LAYERS:].parameters(), "lr": BASE_LR * 0.1, "weight_decay": WEIGHT_DECAY},
        {"params": model.backbone.pos_embedding, "lr": BASE_LR, "weight_decay": WEIGHT_DECAY},
        {"params": model.detection_head.parameters(), "lr": BASE_LR, "weight_decay": WEIGHT_DECAY}
    ])

    criterion = DetectionLoss(warmup_epochs=WARMUP_EPOCHS).to(DEVICE)
    scaler = GradScaler(device=DEVICE)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=BASE_LR * 0.01)

    best_val_loss = float("inf")
    for epoch in range(EPOCHS):
        model.train()
        train_total_loss = 0.0
        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [è®­ç»ƒ]")
        optimizer.zero_grad()

        for step, (images, targets) in enumerate(train_pbar):
            images = images.to(DEVICE, non_blocking=True)
            targets = [t.to(DEVICE, non_blocking=True) for t in targets]

            with autocast(device_type=DEVICE.type):
                _, raw_preds = model(images)
                loss = criterion(raw_preds, targets, epoch) / GRADIENT_ACCUMULATION_STEPS

            scaler.scale(loss).backward()

            if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()

            train_total_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS
            train_pbar.set_postfix({"train_loss": f"{loss.item() * GRADIENT_ACCUMULATION_STEPS:.4f}"})

        avg_train_loss = train_total_loss / len(train_loader)
        current_lr = optimizer.param_groups[0]["lr"]
        print(f"Epoch {epoch+1}/{EPOCHS} | è®­ç»ƒæŸå¤±ï¼š{avg_train_loss:.4f} | å­¦ä¹ ç‡ï¼š{current_lr:.6f}")

        model.eval()
        val_total_loss = 0.0
        with torch.no_grad():
            for images, targets in tqdm(val_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [éªŒè¯]"):
                images = images.to(DEVICE, non_blocking=True)
                targets = [t.to(DEVICE, non_blocking=True) for t in targets]
                with autocast(device_type=DEVICE.type):
                    _, raw_preds = model(images)
                    loss = criterion(raw_preds, targets, epoch)
                val_total_loss += loss.item()

        avg_val_loss = val_total_loss / len(val_loader)
        print(f"Epoch {epoch+1}/{EPOCHS} | éªŒè¯æŸå¤±ï¼š{avg_val_loss:.4f}")

        torch.save(model.state_dict(), LAST_MODEL_PATH)
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), BEST_MODEL_PATH)
            print(f"âœ… æœ€ä¼˜æ¨¡å‹æ›´æ–°ï¼ï¼ˆéªŒè¯æŸå¤±ï¼š{best_val_loss:.4f}ï¼Œä¿å­˜è·¯å¾„ï¼š{BEST_MODEL_PATH}ï¼‰")

        scheduler.step()

    print(f"\nğŸ‰ è®­ç»ƒå…¨éƒ¨å®Œæˆï¼")
    print(f"ğŸ“Œ æœ€ä¼˜æ¨¡å‹è·¯å¾„ï¼š{BEST_MODEL_PATH}ï¼ˆéªŒè¯æŸå¤±ï¼š{best_val_loss:.4f}ï¼‰")


if __name__ == "__main__":
    if not torch.cuda.is_available():
        print("âš ï¸ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°GPUï¼")
        input("æŒ‰Enterç»§ç»­ï¼ˆä¸æ¨èï¼‰ï¼Œæˆ–Ctrl+Cç»ˆæ­¢ï¼š")
    train()