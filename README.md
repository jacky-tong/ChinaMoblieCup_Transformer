# ChinaMoblieCup_Transformer
# ViT目标检测模型实现说明文档


## 项目概述
本项目基于Vision Transformer（ViT）架构实现了多个版本的目标检测模型，包含`ViT.py`、`ViT2_ez.py`、`ViT3_base.py`三个核心文件。模型通过将图像分割为补丁（Patch），利用Transformer提取全局特征，结合自定义检测头实现目标边界框和类别的预测，适用于通用目标检测任务。


## 文件结构与核心功能
### 1. `ViT3_base.py`
#### 核心组件
- **`CustomDetectionDataset` 类**：自定义数据集加载器，负责：
  - 加载图像（支持`jpg`/`png`格式，兼容大小写）和对应标签（`txt`格式）
  - 图像预处理（尺寸调整、转Tensor、标准化）
  - 标签格式转换（将相对坐标转为绝对坐标，处理无目标样本）

- **`DetectionHead` 类**：目标检测头，由三个子网络组成：
  - 边界框预测头：通过全连接层预测边界框参数（输出范围0-1）
  - 类别预测头：预测目标类别（含背景类）
  - 置信度预测头：预测目标存在的置信度（输出范围0-1）

- **`train_model` 函数**：主训练流程，包含：
  - 数据加载器初始化
  - 模型、优化器、调度器、损失函数配置
  - 训练/验证循环（记录损失、保存最佳模型和检查点）
  - 训练日志保存（损失曲线、最佳验证损失）


### 2. `ViT.py`
#### 核心组件
- **`nms_boxes` 函数**：非极大值抑制（NMS）后处理，用于去除重复预测框：
  - 按置信度降序排序预测框
  - 迭代保留高置信度框，过滤与当前框IoU超过阈值的框

- **ViT主干网络 `forward` 方法**：
  - 图像补丁嵌入（将图像转为补丁序列并映射到特征维度）
  - 拼接类别令牌（cls_token）和位置编码
  - Transformer编码器提取全局特征

- **检测头前向传播**：
  - 将Transformer输出的补丁特征输入检测头，得到原始预测（边界框参数、置信度、类别logits）
  - 边界框参数转换（相对坐标→绝对坐标）
  - 置信度与类别分数融合（最终置信度=置信度×类别最高分）

- **损失函数 `forward` 方法**：
  - 处理无真实目标样本（仅优化置信度损失）
  - 计算预测框与真实框的IoU矩阵，实现正负样本匹配（每个真实框匹配IoU最大的预测框）
  - 多损失融合：边界框损失（SmoothL1）、类别损失（CrossEntropy）、置信度损失（BCEWithLogits）

- **`calculate_simple_map` 函数**：简化版mAP计算（平均精度均值），用于模型评估：
  - 按类别统计预测框与真实框
  - 计算真正例（TP）、假正例（FP），生成Precision-Recall曲线
  - 梯形积分计算AP，平均所有类别AP得到mAP


### 3. `ViT2_ez.py`
#### 核心改进与功能
- **窗口注意力机制**：在Transformer编码器中引入窗口注意力（`WindowAttention`类）：
  - 对补丁序列分窗口计算注意力，减少计算量
  - 支持补丁序列长度补零以适配窗口大小
  - 窗口内注意力计算与特征重组

- **简化版检测流程**：
  - 优化补丁特征处理（分离cls_token与补丁令牌，仅对补丁令牌应用Transformer层）
  - 检测头输出格式简化（固定维度为9，适配特定类别数）
  - 损失函数实现更简洁（明确正负样本掩码处理，优化设备兼容性）

- **训练流程优化**：
  - 增强GPU检测与提示（训练前检查CUDA可用性）
  - 简化参数配置，降低使用门槛


## 模型原理
### 1. 整体流程
1. **图像预处理**：图像 resize 至固定尺寸，转换为Tensor并标准化
2. **补丁嵌入**：将图像分割为`16×16`或`32×32`的补丁，通过线性层映射为特征向量
3. **Transformer编码**：
   - 拼接cls_token（用于全局特征聚合）
   - 添加位置编码（注入空间位置信息）
   - 通过Transformer编码器（自注意力/窗口注意力）提取全局特征
4. **检测头预测**：对每个补丁特征预测：
   - 边界框参数（中心坐标、宽高）
   - 目标置信度（是否包含目标）
   - 类别概率（目标所属类别）
5. **后处理**：
   - 边界框坐标转换（相对→绝对）
   - NMS去除重复框
   - 置信度筛选（保留高置信度预测）


### 2. 损失计算
总损失 = 边界框损失×λ_box + 类别损失×λ_cls + 置信度损失  
- **边界框损失**：SmoothL1损失（对异常值更鲁棒），仅计算正样本（匹配到真实框的预测）
- **类别损失**：交叉熵损失，仅计算正样本的类别预测
- **置信度损失**：二分类交叉熵（BCE），正样本目标为1，负样本目标为0


## 使用方法
### 1. 环境依赖
- Python 3.8+
- PyTorch 1.10+
- OpenCV-python
- NumPy
- 可选：CUDA 11.0+（加速训练）


### 2. 数据集准备
- 目录结构：
  ```
  dataset_root/
  ├── images/
  │   ├── train/  # 训练图像（jpg/png）
  │   └── val/    # 验证图像（jpg/png）
  └── labels/
      ├── train/  # 训练标签（txt，与图像同名）
      └── val/    # 验证标签（txt，与图像同名）
  ```
- 标签格式（YOLO格式）：每个txt文件每行表示一个目标，格式为 `cls_id cx cy bw bh`（相对坐标，范围0-1）
  - `cls_id`：类别索引（整数）
  - `cx, cy`：目标中心相对坐标
  - `bw, bh`：目标宽高相对坐标


### 3. 模型训练
#### 运行方式
- 直接运行对应版本的主函数：
  ```bash
  # 运行ViT3_base版本
  python ViT3_base.py

  # 运行ViT版本
  python ViT.py

  # 运行ViT2_ez版本
  python ViT2_ez.py
  ```

#### 训练配置（可在代码中修改）
- `IMG_SIZE`：输入图像尺寸（如`(832, 1472)`）
- `PATCH_SIZE`：补丁大小（如32）
- `EPOCHS`：训练轮数
- `BATCH_SIZE`：批次大小
- `LOSS_LAMBDA_BOX`/`LOSS_LAMBDA_CLS`：损失权重
- `BEST_MODEL_PATH`：最佳模型保存路径


### 4. 模型评估
- 训练过程中自动计算验证损失，保存最佳模型
- 可调用`calculate_simple_map`函数计算mAP（需传入预测结果和真实标签）：
  ```python
  # 示例
  map_score = calculate_simple_map(all_preds, all_targets, iou_threshold=0.5)
  print(f"mAP@0.5: {map_score:.4f}")
  ```


## 注意事项
1. 确保数据集路径正确，图像与标签文件一一对应
2. 无目标的图像需对应空标签文件（或代码自动处理为`[0,0,0,0,-1]`）
3. 训练大型模型建议使用GPU（显存≥8GB）
4. 可通过调整`PATCH_SIZE`、`EPOCHS`、学习率等参数优化性能
5. 模型输出预测框格式为`[x1, y1, x2, y2, confidence, cls_id]`（绝对像素坐标）


## 版本差异说明
| 版本         | 核心特点                     | 适用场景                 |
|--------------|------------------------------|--------------------------|
| `ViT.py`     | 基础ViT架构，完整损失与评估  | 通用目标检测，需要标准评估 |
| `ViT2_ez.py` | 窗口注意力，简化流程        | 轻量化部署，快速训练     |
| `ViT3_base.py` | 模块化设计，完善数据集处理  | 工程化应用，定制化需求   |


### 补充说明

#### 1. 模型结构细节
本项目基于Vision Transformer（ViT）构建目标检测模型，核心结构包括：
- **Backbone**：采用预训练的`vit_b_16`作为特征提取网络，支持加载本地权重或在线下载ImageNet预训练权重
- **检测头（DetectionHead）**：由三个子网络组成
  - 边界框预测头：输出归一化的边界框坐标（x1, y1, x2, y2）
  - 类别预测头：输出目标类别概率（含背景类）
  - 置信度预测头：输出预测框的置信度分数
- **层冻结机制**：支持冻结patch嵌入层及前N层Transformer，仅训练剩余层以减少计算量（通过`_freeze_layers`方法实现）


#### 2. 训练关键优化
- **学习率调度**：采用warmup+余弦退火策略
  - Warmup阶段：从1e-7线性升温至基础学习率（`BASE_LR`）
  - 正式训练：使用余弦退火将学习率从`BASE_LR`衰减至`BASE_LR * 0.01`
- **损失函数**：多组件加权损失
  - 边界框损失：采用SmoothL1Loss，对小目标（面积<阈值）增加权重（`SMALL_OBJ_WEIGHT`）
  - 类别损失：CrossEntropyLoss
  - 置信度损失：BinaryCrossEntropyLoss（匹配目标为1，未匹配为0）
- **梯度累积**：支持通过`GRADIENT_ACCUMULATION_STEPS`参数实现梯度累积，模拟大批次训练效果


#### 3. 数据处理
- **数据集类（CustomDetectionDataset）**：
  - 支持多种图像格式（jpg、png等）及大小写文件名兼容
  - 训练集增强：随机裁剪、水平翻转、色彩抖动，针对小目标增加优先裁剪策略
  - 验证集处理：固定尺寸缩放+中心裁剪，保证输入一致性
  - 标签处理：将归一化坐标转换为绝对坐标，支持空标签处理
- **数据加载优化**：
  - 采用`persistent_workers=True`复用数据加载进程，减少重复初始化开销
  - 动态调整`num_workers`数量（不超过CPU核心数的1/4且≤4）
  - 自定义`collate_fn`处理不同数量目标的样本，避免维度混乱


#### 4. 评估流程
- **指标计算**：通过`pred.py`实现mAP评估，包括
  - mAP50：IoU=0.5时的平均精度
  - mAP50-95：IoU从0.5到0.95（步长0.05）的平均精度
  - 各类别在IoU=0.5时的AP值
- **标签加载**：支持区分真实标签（5值格式）和预测标签（6值格式，含置信度），自动转换归一化坐标为绝对坐标
- **非极大值抑制（NMS）**：在`vote_config.py`中实现，用于过滤冗余预测框（默认IoU阈值0.5）


#### 5. 关键参数配置
核心配置参数可通过全局变量或配置文件调整，主要包括：
- 模型参数：`IMG_SIZE`（输入尺寸）、`FREEZE_LAYERS`（冻结层数）、`NUM_CLASSES`（目标类别数）
- 训练参数：`BATCH_SIZE`、`EPOCHS`、`WARMUP_EPOCHS`、`BASE_LR`、`WEIGHT_DECAY`
- 数据参数：`SMALL_OBJ_AREA_THR`（小目标面积阈值）、`SMALL_OBJ_WEIGHT`（小目标损失权重）


#### 6. 注意事项
- 权重加载：若存在本地ViT权重（`LOCAL_VIT_WEIGHTS_PATH`），将优先加载；否则自动下载ImageNet预训练权重
- 空标签处理：训练时对无目标样本仅计算背景置信度损失，避免无效梯度
- 设备兼容：自动使用配置的`DEVICE`（GPU/CPU）进行训练和推理，支持非阻塞数据传输（`non_blocking=True`）加速
- 模型保存：训练过程中自动保存最佳模型（基于验证损失）和每10轮的检查点，存储路径为`TRAIN_OUTPUT_DIR`
