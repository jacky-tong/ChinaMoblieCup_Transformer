import os
import yaml
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from pathlib import Path
import psutil
import cv2
import numpy as np
from tqdm import tqdm
from einops import rearrange, repeat
from einops.layers.torch import Rearrange
import shutil
from torch.amp import autocast, GradScaler  # ä¿®å¤APIï¼šæ”¹ç”¨torch.amp
from torch.utils.checkpoint import checkpoint  # æ¢¯åº¦æ£€æŸ¥ç‚¹

# ============================== å…¨å±€é…ç½®ï¼ˆä¿æŒæ¨¡å‹ç»´åº¦ä¸å˜ï¼‰==============================
DATASET_ROOT = r"E:\2025CompetitionLog\9.9\tRVALCUP_2\ChinaMoblieCup\dataset" 
TRAIN_OUTPUT_DIR = "./runs/vit_detect_optimized"
MODEL_NAME = "vit_detector_4090_optimized"
BEST_MODEL_PATH = os.path.join(TRAIN_OUTPUT_DIR, MODEL_NAME, "weights", "best.pt")
VAL_IMAGES_PATH = os.path.join(DATASET_ROOT, "images", "val")
VAL_LABELS_PATH = os.path.join(DATASET_ROOT, "labels", "val")

# æ¨¡å‹è¶…å‚æ•°ï¼ˆä¿æŒåŸæœ‰ç»´åº¦ï¼Œä¿®å¤çª—å£å¤§å°ï¼‰
IMG_SIZE = (832, 1472)
PATCH_SIZE = 32
DIM = 512
DEPTH = 4
HEADS = 8
MLP_DIM = 1024
NUM_CLASSES = 4
WINDOW_SIZE = 10  # ä¿®å¤ï¼š10Ã—10=100ï¼Œ1196ï¼ˆæ€»è¡¥ä¸æ•°ï¼‰å¯é€‚é…çª—å£åˆ’åˆ†

# è®­ç»ƒè¶…å‚æ•°ï¼ˆ4090ä¼˜åŒ–ç‰ˆï¼‰
EPOCHS = 200
BATCH_SIZE = 2               
GRADIENT_ACCUMULATION_STEPS = 8  # ç­‰æ•ˆ batch_size=16
BASE_LR = 5e-5
WEIGHT_DECAY = 1e-4
WARMUP_EPOCHS = 3
LOSS_LAMBDA_BOX = 5.0
LOSS_LAMBDA_CLS = 1.0

# æ¨ç†è¶…å‚æ•°
CONF_THRESHOLD = 0.5
IOU_THRESHOLD = 0.5

DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
GPU_MEMORY_GB = torch.cuda.get_device_properties(0).total_memory / (1024**3) if torch.cuda.is_available() else 0

# ============================== å·¥å…·å‡½æ•° ==============================
def nms_boxes(boxes, iou_threshold):
    if len(boxes) == 0:
        return []
    boxes = sorted(boxes, key=lambda x: x[4], reverse=True)
    keep_boxes = []
    while boxes:
        current_box = boxes.pop(0)
        keep_boxes.append(current_box)
        boxes = [
            box for box in boxes
            if calculate_iou(np.array(current_box[:4]), np.array(box[:4])) < iou_threshold
        ]
    return keep_boxes

def calculate_iou(box1, box2):
    x1_max = max(box1[0], box2[0])
    y1_max = max(box1[1], box2[1])
    x2_min = min(box1[2], box2[2])
    y2_min = min(box1[3], box2[3])
    if x2_min <= x1_max or y2_min <= y1_max:
        return 0.0
    intersection = (x2_min - x1_max) * (y2_min - y1_max)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - intersection
    return intersection / union if union > 0 else 0.0

# ============================== çª—å£æ³¨æ„åŠ›æ¨¡å—ï¼ˆä¿®å¤å°ºå¯¸åŒ¹é…ï¼‰==============================
class WindowAttention(nn.Module):
    def __init__(self, dim, heads, window_size):
        super().__init__()
        self.dim = dim
        self.heads = heads
        self.window_size = window_size
        self.scale = (dim // heads) ** -0.5
        self.qkv = nn.Linear(dim, dim * 3)
        self.proj = nn.Linear(dim, dim)

    def forward(self, x):
        B, N, C = x.shape
        ws2 = self.window_size ** 2
        # è®¡ç®—æ‰€éœ€è¡¥é›¶ï¼ˆæ›´ç¨³å¥çš„å–æ¨¡æ–¹å¼ï¼Œç»“æœåœ¨[0, ws2-1]ï¼‰
        pad_num = (ws2 - (N % ws2)) % ws2
        if pad_num > 0:
            x = F.pad(x, (0, 0, 0, pad_num))  # (B, N+pad, C)
        N_pad = x.shape[1]
        # è¡¥é›¶åé‡æ–°è®¡ç®—çª—å£æ•°ï¼Œç¡®ä¿ rearrange æ—¶ä¸€è‡´
        num_windows = N_pad // ws2

        # çª—å£æ³¨æ„åŠ›è®¡ç®—
        x = rearrange(x, 'b (nw ws2) c -> b nw ws2 c', nw=num_windows, ws2=ws2)
        qkv = self.qkv(x).reshape(B, num_windows, ws2, 3, self.heads, C // self.heads).permute(3, 0, 1, 4, 2, 5)
        q, k, v = qkv[0], qkv[1], qkv[2]

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        x = (attn @ v).transpose(3, 4).reshape(B, num_windows, ws2, C)
        x = rearrange(x, 'b nw ws2 c -> b (nw ws2) c')

        # è£å‰ªå›åŸå§‹é•¿åº¦ï¼ˆå»é™¤è¡¥é›¶ï¼‰
        if pad_num > 0:
            x = x[:, :N, :]

        x = self.proj(x)
        return x

# ============================== æ¨¡å‹å®šä¹‰ï¼ˆä¿®å¤æ¢¯åº¦æ£€æŸ¥ç‚¹å‚æ•°ï¼‰==============================
class ViTBackbone(nn.Module):
    def __init__(self, img_size=IMG_SIZE, patch_size=PATCH_SIZE, dim=DIM, depth=DEPTH, heads=HEADS, mlp_dim=MLP_DIM):
        super().__init__()
        image_height, image_width = img_size
        patch_height, patch_width = patch_size, patch_size
        self.num_patches = (image_height // patch_height) * (image_width // patch_width)  # 26Ã—46=1196
        self.patch_dim = 3 * patch_height * patch_width

        self.to_patch_embedding = nn.Sequential(
            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width),
            nn.Linear(self.patch_dim, dim),
        )

        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, dim))  # +1 for cls_token
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))

        # çª—å£æ³¨æ„åŠ›å±‚ï¼ˆä½¿ç”¨ä¿®å¤åçš„WindowAttentionï¼‰
        self.transformer_layers = nn.ModuleList([
            nn.Sequential(
                nn.LayerNorm(dim),
                WindowAttention(dim, heads, WINDOW_SIZE),
                nn.LayerNorm(dim),
                nn.Sequential(
                    nn.Linear(dim, mlp_dim),
                    nn.GELU(),
                    nn.Linear(mlp_dim, dim)
                )
            )
            for _ in range(depth)
        ])

    def forward(self, img):
        batch_size = img.shape[0]
        # 1. è¡¥ä¸åµŒå…¥
        x = self.to_patch_embedding(img)  # (B, 1196, 512)
        # 2. æ·»åŠ cls_token
        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=batch_size)  # (B, 1, 512)
        x = torch.cat([cls_tokens, x], dim=1)  # (B, 1197, 512)
        # 3. æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_embedding  # (B, 1197, 512)

        # 4. Transformerç¼–ç ï¼šå¯¹patch tokensï¼ˆä¸å«clsï¼‰é€å±‚åº”ç”¨çª—å£æ³¨æ„åŠ›ï¼ˆä¿æŒclsä¸å˜ï¼‰
        cls_token = x[:, :1, :]           # (B,1,dim)
        patch_tokens = x[:, 1:, :]        # (B, num_patches, dim)

        for blk in self.transformer_layers:
            # åªå¯¹ patch_tokens åº”ç”¨å—ï¼ˆblk åŒ…å« LayerNorm, WindowAttention, ...ï¼‰
            patch_tokens = checkpoint(blk, patch_tokens, use_reentrant=False)

        # æ‹¼å› cls_token ä¸ patch_tokens
        x = torch.cat([cls_token, patch_tokens], dim=1)  # (B, 1197, dim)
        return x

class ViTDetector(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super().__init__()
        self.backbone = ViTBackbone()
        self.detection_head = nn.Sequential(
            nn.Linear(DIM, DIM // 2),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(DIM // 2, 5 + num_classes)  # 4åæ ‡+1ç½®ä¿¡åº¦+4ç±»åˆ«
        )
        self._initialize_weights()

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.trunc_normal_(m.weight, std=.02)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.bias, 0)
                nn.init.constant_(m.weight, 1.0)

    def forward(self, x):
        # 1. æå–ViTç‰¹å¾
        features = self.backbone(x)  # (B, 1197, 512)
        patch_features = features[:, 1:]  # å»é™¤cls_tokenï¼š(B, 1196, 512)
        # 2. æ£€æµ‹å¤´é¢„æµ‹
        detections = self.detection_head(patch_features)  # (B, 1196, 9)

        # 3. åæ ‡è½¬æ¢ï¼ˆç»å¯¹åƒç´ åæ ‡ï¼‰
        h, w = IMG_SIZE
        bboxes = detections[..., :4].clone()
        # x_center (ç›¸å¯¹â†’ç»å¯¹)
        bboxes[..., 0] = (bboxes[..., 0] * 2 - 1) * w / 2
        # y_center (ç›¸å¯¹â†’ç»å¯¹)
        bboxes[..., 1] = (bboxes[..., 1] * 2 - 1) * h / 2
        # width (æŒ‡æ•°è¿˜åŸâ†’ç»å¯¹)
        bboxes[..., 2] = torch.exp(bboxes[..., 2]) * (w / (w // PATCH_SIZE))
        # height (æŒ‡æ•°è¿˜åŸâ†’ç»å¯¹)
        bboxes[..., 3] = torch.exp(bboxes[..., 3]) * (h / (h // PATCH_SIZE))

        # è½¬æ¢ä¸ºx1,y1,x2,y2æ ¼å¼
        x1 = bboxes[..., 0] - bboxes[..., 2] / 2
        y1 = bboxes[..., 1] - bboxes[..., 3] / 2
        x2 = bboxes[..., 0] + bboxes[..., 2] / 2
        y2 = bboxes[..., 1] + bboxes[..., 3] / 2

        # 4. ç½®ä¿¡åº¦ä¸ç±»åˆ«å¤„ç†
        class_logits = detections[..., 5:]  # (B, 1196, 4)
        confidences = torch.sigmoid(detections[..., 4:5])  # ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
        class_scores = torch.max(F.softmax(class_logits, dim=-1), dim=-1, keepdim=True).values  # ç±»åˆ«æœ€é«˜åˆ†
        final_confidence = confidences * class_scores  # æœ€ç»ˆç½®ä¿¡åº¦ï¼ˆç½®ä¿¡åº¦Ã—ç±»åˆ«åˆ†æ•°ï¼‰
        class_ids = torch.argmax(F.softmax(class_logits, dim=-1), dim=-1, keepdim=True)  # ç±»åˆ«ID

        # 5. æ‹¼æ¥æœ€ç»ˆè¾“å‡º
        processed_outputs = torch.cat([
            x1.unsqueeze(-1), y1.unsqueeze(-1), x2.unsqueeze(-1), y2.unsqueeze(-1),
            final_confidence, class_ids.float()
        ], dim=-1)  # (B, 1196, 6)ï¼šx1,y1,x2,y2,conf,cls_id

        return processed_outputs, detections

# ============================== æŸå¤±å‡½æ•°ï¼ˆè¡¥å…¨å®Œæ•´é€»è¾‘ï¼‰==============================
class DetectionLoss(nn.Module):
    def __init__(self, lambda_box=LOSS_LAMBDA_BOX, lambda_cls=LOSS_LAMBDA_CLS):
        super().__init__()
        self.lambda_box = lambda_box
        self.lambda_cls = lambda_cls
        # åŸºç¡€æŸå¤±å‡½æ•°
        self.bbox_loss = nn.SmoothL1Loss(reduction='sum')  # è¾¹ç•Œæ¡†æŸå¤±ï¼ˆå¯¹å¼‚å¸¸å€¼é²æ£’ï¼‰
        self.class_loss = nn.CrossEntropyLoss(reduction='sum')  # ç±»åˆ«æŸå¤±
        self.confidence_loss = nn.BCEWithLogitsLoss(reduction='sum')  # ç½®ä¿¡åº¦æŸå¤±ï¼ˆäºŒåˆ†ç±»ï¼‰

    def _get_img_wh(self):
        # è¿”å›å›¾åƒå®½é«˜ï¼ˆä¸æ¨¡å‹è¾“å…¥ä¸€è‡´ï¼‰
        return IMG_SIZE[1], IMG_SIZE[0]  # (w, h)

    def forward(self, predictions, targets):
        """
        predictions: æ£€æµ‹å¤´åŸå§‹è¾“å‡º (B, 1196, 9)
        targets: çœŸå®æ ‡ç­¾åˆ—è¡¨ (Bä¸ªå…ƒç´ ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯(num_objects, 5)ï¼šx1,y1,x2,y2,cls_id)
        """
        total_loss = 0.0
        batch_size = predictions.shape[0]
        w_img, h_img = self._get_img_wh()

        for i in range(batch_size):
            pred = predictions[i]  # (1196, 9)
            target = targets[i]    # (num_objects, 5)

            # æƒ…å†µ1ï¼šå½“å‰æ ·æœ¬æ— çœŸå®ç›®æ ‡â†’ä»…ä¼˜åŒ–ç½®ä¿¡åº¦ï¼ˆå¸Œæœ›ç½®ä¿¡åº¦ä¸º0ï¼‰
            if target.numel() == 0:
                conf_loss = self.confidence_loss(pred[:, 4], torch.zeros_like(pred[:, 4], device=pred.device))
                total_loss += conf_loss
                continue

            # æƒ…å†µ2ï¼šæœ‰çœŸå®ç›®æ ‡â†’åŒ¹é…é¢„æµ‹æ¡†ä¸çœŸå®æ¡†
            # 2.1 é¢„æµ‹æ¡†åæ ‡è½¬æ¢ï¼ˆåŸå§‹å‚æ•°â†’ç»å¯¹åæ ‡x1,y1,x2,y2ï¼‰
            pred_bbox = pred[:, :4].clone()
            # x_center
            pred_bbox[:, 0] = (pred_bbox[:, 0] * 2 - 1) * w_img / 2
            # y_center
            pred_bbox[:, 1] = (pred_bbox[:, 1] * 2 - 1) * h_img / 2
            # width
            pred_bbox[:, 2] = torch.exp(pred_bbox[:, 2]) * (w_img / (w_img // PATCH_SIZE))
            # height
            pred_bbox[:, 3] = torch.exp(pred_bbox[:, 3]) * (h_img / (h_img // PATCH_SIZE))
            # è½¬æ¢ä¸ºx1,y1,x2,y2
            pred_x1 = pred_bbox[:, 0] - pred_bbox[:, 2] / 2
            pred_y1 = pred_bbox[:, 1] - pred_bbox[:, 3] / 2
            pred_x2 = pred_bbox[:, 0] + pred_bbox[:, 2] / 2
            pred_y2 = pred_bbox[:, 1] + pred_bbox[:, 3] / 2
            pred_boxes_abs = torch.stack([pred_x1, pred_y1, pred_x2, pred_y2], dim=1)  # (1196, 4)

            # 2.2 è®¡ç®—IoUçŸ©é˜µï¼ˆé¢„æµ‹æ¡† vs çœŸå®æ¡†ï¼‰
            num_preds = pred_boxes_abs.shape[0]
            num_tgts = target.shape[0]
            ious = torch.zeros((num_preds, num_tgts), device=pred.device)
            for p_idx in range(num_preds):
                for t_idx in range(num_tgts):
                    # è®¡ç®—å•ä¸ªIoUï¼ˆé¿å…ç±»å‹ä¸å…¼å®¹ï¼‰
                    p_box = pred_boxes_abs[p_idx].detach().cpu().numpy()
                    t_box = target[t_idx, :4].detach().cpu().numpy()
                    iou_val = calculate_iou(p_box, t_box)
                    ious[p_idx, t_idx] = torch.tensor(iou_val, device=pred.device)

            # 2.3 åŒ¹é…ç­–ç•¥ï¼šæ¯ä¸ªçœŸå®æ¡†åŒ¹é…IoUæœ€å¤§çš„é¢„æµ‹æ¡†ï¼ˆæ­£æ ·æœ¬ï¼‰
            matched_pred_ids = torch.argmax(ious, dim=0)  # (num_tgts,)ï¼šæ¯ä¸ªçœŸå®æ¡†å¯¹åº”çš„é¢„æµ‹æ¡†ID
            matched_tgt_ids = torch.arange(num_tgts, device=pred.device)  # (num_tgts,)

            # 2.4 ç”Ÿæˆæ­£è´Ÿæ ·æœ¬æ©ç 
            positive_mask = torch.zeros(num_preds, dtype=torch.bool, device=pred.device)
            positive_mask[matched_pred_ids] = True  # æ­£æ ·æœ¬ï¼šåŒ¹é…åˆ°çœŸå®æ¡†çš„é¢„æµ‹æ¡†
            negative_mask = ~positive_mask  # è´Ÿæ ·æœ¬ï¼šæœªåŒ¹é…çš„é¢„æµ‹æ¡†

            # 2.5 è®¡ç®—å„éƒ¨åˆ†æŸå¤±
            # è¾¹ç•Œæ¡†æŸå¤±ï¼ˆä»…æ­£æ ·æœ¬ï¼‰
            pred_matched_boxes = pred_boxes_abs[matched_pred_ids]  # (num_tgts, 4)
            tgt_matched_boxes = target[matched_tgt_ids, :4]  # (num_tgts, 4)
            box_loss = self.bbox_loss(pred_matched_boxes, tgt_matched_boxes)

            # ç±»åˆ«æŸå¤±ï¼ˆä»…æ­£æ ·æœ¬ï¼‰
            pred_matched_cls = pred[matched_pred_ids, 5:5+NUM_CLASSES]  # (num_tgts, 4)
            tgt_matched_cls = target[matched_tgt_ids, 4].long()  # (num_tgts,)
            cls_loss = self.class_loss(pred_matched_cls, tgt_matched_cls)

            # ç½®ä¿¡åº¦æŸå¤±ï¼ˆæ­£æ ·æœ¬â†’1ï¼Œè´Ÿæ ·æœ¬â†’0ï¼‰
            conf_targets = torch.zeros(num_preds, device=pred.device)
            conf_targets[positive_mask] = 1.0
            conf_loss = self.confidence_loss(pred[:, 4], conf_targets)

            # 2.6 æ€»æŸå¤±ï¼ˆåŠ æƒæ±‚å’Œï¼‰
            total_loss += self.lambda_box * box_loss + self.lambda_cls * cls_loss + conf_loss

        # å¹³å‡åˆ°æ¯ä¸ªæ ·æœ¬
        return total_loss / batch_size

# ============================== æ•°æ®é›†ï¼ˆä¿®å¤å›¾ç‰‡æ ¼å¼ä¸æ ‡ç­¾è½¬æ¢ï¼‰==============================
class CustomDetectionDataset(Dataset):
    def __init__(self, img_dir, label_dir, img_size=IMG_SIZE):
        self.img_dir = Path(img_dir)
        self.label_dir = Path(label_dir)
        self.img_size = img_size  # (h, w)
        self.h_origin = None  # åŸå§‹å›¾åƒé«˜åº¦
        self.w_origin = None  # åŸå§‹å›¾åƒå®½åº¦

        # æ•°æ®é¢„å¤„ç†ï¼ˆä¸æ¨¡å‹è¾“å…¥åŒ¹é…ï¼‰
        self.transform = transforms.Compose([
            transforms.ToTensor(),  # HWCâ†’CHWï¼Œåƒç´ å½’ä¸€åŒ–åˆ°[0,1]
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNetæ ‡å‡†åŒ–
        ])

        # ä¿®å¤ï¼šæ”¶é›†æ‰€æœ‰å¸¸è§å›¾ç‰‡æ ¼å¼
        self.images = []
        for ext in ['.jpg', '.jpeg', '.png', '.bmp', '.JPG', '.JPEG', '.PNG']:
            self.images.extend(list(self.img_dir.glob(f'*{ext}')))
        if len(self.images) == 0:
            raise FileNotFoundError(f"åœ¨{img_dir}æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶ï¼ˆæ”¯æŒæ ¼å¼ï¼šjpg/jpeg/png/bmpï¼‰")

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path = self.images[idx]
        label_path = self.label_dir / (img_path.stem + '.txt')  # æ ‡ç­¾æ–‡ä»¶ä¸å›¾ç‰‡åŒå

        # 1. åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ
        image = cv2.imread(str(img_path))
        if image is None:
            raise ValueError(f"æ— æ³•åŠ è½½å›¾åƒï¼š{img_path}")
        self.h_origin, self.w_origin = image.shape[:2]  # è®°å½•åŸå§‹å°ºå¯¸
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # BGRâ†’RGBï¼ˆcv2é»˜è®¤BGRï¼‰
        image_resized = cv2.resize(image_rgb, (self.img_size[1], self.img_size[0]))  # ç¼©æ”¾åˆ°æ¨¡å‹è¾“å…¥å°ºå¯¸
        image_tensor = self.transform(image_resized)  # (3, 832, 1472)

        # 2. åŠ è½½å¹¶è½¬æ¢æ ‡ç­¾ï¼ˆYOLOæ ¼å¼â†’ç»å¯¹åæ ‡ï¼‰
        targets = []
        if label_path.exists():
            with open(label_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    parts = line.split()
                    if len(parts) != 5:
                        continue  # è·³è¿‡æ ¼å¼é”™è¯¯çš„è¡Œ

                    # YOLOæ ¼å¼ï¼šcls_id, cx_rel, cy_rel, bw_rel, bh_relï¼ˆç›¸å¯¹åæ ‡ï¼‰
                    cls_id, cx_rel, cy_rel, bw_rel, bh_rel = map(float, parts)

                    # è½¬æ¢ä¸ºåŸå§‹å›¾åƒçš„ç»å¯¹åæ ‡ï¼ˆx1,y1,x2,y2ï¼‰
                    cx_abs = cx_rel * self.w_origin
                    cy_abs = cy_rel * self.h_origin
                    bw_abs = bw_rel * self.w_origin
                    bh_abs = bh_rel * self.h_origin
                    x1_abs = cx_abs - bw_abs / 2
                    y1_abs = cy_abs - bh_abs / 2
                    x2_abs = cx_abs + bw_abs / 2
                    y2_abs = cy_abs + bh_abs / 2

                    # ç¼©æ”¾åˆ°æ¨¡å‹è¾“å…¥å°ºå¯¸çš„ç»å¯¹åæ ‡
                    scale_x = self.img_size[1] / self.w_origin
                    scale_y = self.img_size[0] / self.h_origin
                    x1 = x1_abs * scale_x
                    y1 = y1_abs * scale_y
                    x2 = x2_abs * scale_x
                    y2 = y2_abs * scale_y

                    targets.append([x1, y1, x2, y2, cls_id])

        # è½¬æ¢ä¸ºTensorï¼ˆæ— ç›®æ ‡æ—¶è¿”å›ç©ºTensorï¼‰
        targets_tensor = torch.tensor(targets, dtype=torch.float32) if targets else torch.empty((0, 5), dtype=torch.float32)
        return image_tensor, targets_tensor

# ============================== è®­ç»ƒå™¨ï¼ˆä¿®å¤AMPä¸æ•°æ®åŠ è½½ï¼‰==============================
class ViTDetectorTrainer:
    def __init__(self, dataset_root=DATASET_ROOT):
        self.dataset_root = Path(dataset_root)
        # éªŒè¯æ•°æ®é›†ç›®å½•
        self._validate_dataset()
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = Path(TRAIN_OUTPUT_DIR) / MODEL_NAME
        self.weights_dir = self.save_dir / 'weights'
        self.weights_dir.mkdir(parents=True, exist_ok=True)

    def _validate_dataset(self):
        """éªŒè¯æ•°æ®é›†ç›®å½•ç»“æ„"""
        required_dirs = [
            self.dataset_root / 'images' / 'train',
            self.dataset_root / 'images' / 'val',
            self.dataset_root / 'labels' / 'train',
            self.dataset_root / 'labels' / 'val'
        ]
        for dir_path in required_dirs:
            if not dir_path.exists():
                raise FileNotFoundError(f"æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨ï¼š{dir_path}")
        print(f"âœ… æ•°æ®é›†ç›®å½•éªŒè¯é€šè¿‡")

    def collate_fn(self, batch):
        """DataLoader collationï¼šå¤„ç†ä¸åŒæ ·æœ¬ç›®æ ‡æ•°é‡ä¸ä¸€è‡´çš„é—®é¢˜"""
        images, targets = zip(*batch)
        # å›¾åƒå †å ï¼ˆbatch_size, 3, 832, 1472ï¼‰
        images_tensor = torch.stack(images, dim=0)
        # ç›®æ ‡ä¿æŒåˆ—è¡¨ï¼ˆæ¯ä¸ªå…ƒç´ æ˜¯(num_objects, 5)çš„Tensorï¼‰
        return images_tensor, targets

    def train_model(self):
        # 1. åŠ è½½æ•°æ®é›†
        train_dataset = CustomDetectionDataset(
            img_dir=self.dataset_root / 'images' / 'train',
            label_dir=self.dataset_root / 'labels' / 'train'
        )
        val_dataset = CustomDetectionDataset(
            img_dir=self.dataset_root / 'images' / 'val',
            label_dir=self.dataset_root / 'labels' / 'val'
        )

        # ä¿®å¤ï¼šé™åˆ¶num_workersï¼ˆé¿å…CPUçº¿ç¨‹è¿‡å¤šï¼‰
        num_workers = min(psutil.cpu_count() // 2, 8)  # æœ€å¤š8ä¸ªworker
        train_loader = DataLoader(
            train_dataset,
            batch_size=BATCH_SIZE,
            shuffle=True,
            num_workers=num_workers,
            collate_fn=self.collate_fn,
            pin_memory=True,  # åŠ é€ŸGPUæ•°æ®ä¼ è¾“
            drop_last=True  # ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´æ‰¹æ¬¡
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=BATCH_SIZE,
            shuffle=False,
            num_workers=num_workers,
            collate_fn=self.collate_fn,
            pin_memory=True
        )
        print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼ˆè®­ç»ƒé›†ï¼š{len(train_dataset)}å¼ ï¼ŒéªŒè¯é›†ï¼š{len(val_dataset)}å¼ ï¼‰")

        # 2. åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°
        model = ViTDetector().to(DEVICE)
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=BASE_LR,
            weight_decay=WEIGHT_DECAY
        )
        criterion = DetectionLoss().to(DEVICE)
        # ä¿®å¤ï¼šAMPåˆå§‹åŒ–ï¼ˆæŒ‡å®šè®¾å¤‡ä¸º'cuda'ï¼‰
        scaler = GradScaler(device='cuda')
        # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆä½™å¼¦é€€ç«ï¼‰
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=EPOCHS,
            eta_min=BASE_LR * 0.01  # æœ€å°å­¦ä¹ ç‡
        )

        # 3. è®­ç»ƒå¾ªç¯
        best_val_loss = float('inf')
        for epoch in range(EPOCHS):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_total_loss = 0.0
            train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [è®­ç»ƒ]")
            optimizer.zero_grad()  # åˆå§‹åŒ–æ¢¯åº¦

            for step, (images, targets) in enumerate(train_pbar):
                images = images.to(DEVICE, non_blocking=True)  # éé˜»å¡ä¼ è¾“
                targets = [t.to(DEVICE, non_blocking=True) for t in targets]

                # æ··åˆç²¾åº¦è®­ç»ƒ
                with autocast(device_type='cuda'):  # æ˜¾å¼æŒ‡å®šè®¾å¤‡ç±»å‹
                    _, raw_outputs = model(images)
                    loss = criterion(raw_outputs, targets) / GRADIENT_ACCUMULATION_STEPS  # æ¢¯åº¦ç´¯ç§¯ï¼šæŸå¤±å‡åˆ†

                # åå‘ä¼ æ’­ï¼ˆç¼©æ”¾æŸå¤±ï¼Œé¿å…æ¢¯åº¦ä¸‹æº¢ï¼‰
                scaler.scale(loss).backward()

                # æ¢¯åº¦ç´¯ç§¯ï¼šè¾¾åˆ°æ­¥æ•°åæ›´æ–°å‚æ•°
                if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:
                    scaler.step(optimizer)  # æ›´æ–°ä¼˜åŒ–å™¨
                    scaler.update()  # æ›´æ–°ç¼©æ”¾å™¨
                    optimizer.zero_grad()  # é‡ç½®æ¢¯åº¦

                # è®°å½•æŸå¤±
                train_total_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS
                train_pbar.set_postfix({"train_loss": f"{loss.item() * GRADIENT_ACCUMULATION_STEPS:.4f}"})

            # è®­ç»ƒé˜¶æ®µç»Ÿè®¡
            avg_train_loss = train_total_loss / len(train_loader)
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Epoch {epoch+1}/{EPOCHS} | è®­ç»ƒæŸå¤±ï¼š{avg_train_loss:.4f} | å­¦ä¹ ç‡ï¼š{current_lr:.6f}")

            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_total_loss = 0.0
            val_pbar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [éªŒè¯]")
            with torch.no_grad():
                for images, targets in val_pbar:
                    images = images.to(DEVICE, non_blocking=True)
                    targets = [t.to(DEVICE, non_blocking=True) for t in targets]

                    with autocast(device_type='cuda'):
                        _, raw_outputs = model(images)
                        loss = criterion(raw_outputs, targets)

                    val_total_loss += loss.item()
                    val_pbar.set_postfix({"val_loss": f"{loss.item():.4f}"})

            # éªŒè¯é˜¶æ®µç»Ÿè®¡
            avg_val_loss = val_total_loss / len(val_loader)
            print(f"Epoch {epoch+1}/{EPOCHS} | éªŒè¯æŸå¤±ï¼š{avg_val_loss:.4f}")

            # ä¿å­˜æ¨¡å‹
            # 3.1 ä¿å­˜æœ€æ–°æ¨¡å‹
            torch.save(model.state_dict(), self.weights_dir / "last.pt")
            # 3.2 ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆåŸºäºéªŒè¯æŸå¤±ï¼‰
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                torch.save(model.state_dict(), self.weights_dir / "best.pt")
                print(f"âœ… æœ€ä¼˜æ¨¡å‹æ›´æ–°ï¼ˆéªŒè¯æŸå¤±ï¼š{best_val_loss:.4f}ï¼‰")
            # 3.3 æ¯25è½®ä¿å­˜ä¸­é—´æ¨¡å‹
            if (epoch + 1) % 25 == 0:
                torch.save(model.state_dict(), self.weights_dir / f"epoch_{epoch+1}.pt")
                print(f"âœ… ä¸­é—´æ¨¡å‹ä¿å­˜ï¼šepoch_{epoch+1}.pt")

            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()

        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ“ æœ€ä¼˜æ¨¡å‹ï¼š{self.weights_dir / 'best.pt'}ï¼ˆéªŒè¯æŸå¤±ï¼š{best_val_loss:.4f}ï¼‰")
        print(f"ğŸ“ æœ€æ–°æ¨¡å‹ï¼š{self.weights_dir / 'last.pt'}")

# ============================== ä¸»å‡½æ•° ==============================
def main():
    # æ£€æŸ¥GPU
    if not torch.cuda.is_available():
        print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œè®­ç»ƒå°†éå¸¸ç¼“æ…¢ï¼å»ºè®®ä½¿ç”¨CUDAè®¾å¤‡ã€‚")
    else:
        print(f"âœ… æ£€æµ‹åˆ°GPUï¼š{torch.cuda.get_device_name(0)}ï¼ˆæ˜¾å­˜ï¼š{GPU_MEMORY_GB:.1f}GBï¼‰")

    # åˆå§‹åŒ–è®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = ViTDetectorTrainer()
    trainer.train_model()

if __name__ == '__main__':
    main()